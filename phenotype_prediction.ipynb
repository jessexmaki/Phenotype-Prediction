{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozxit2d2jHeI"
      },
      "source": [
        "# Phenotype Prediction Case Study\n",
        "\n",
        "The model organism *Drosophila melanogaster*, commonly known as the fruit fly, has been one of the most important and well-studied systems in biological and biomedical research. Early studies utilizing *D. melanogaster* were instrumental in elucidating foundational principles of genetics and inheritance, including characterization of mutagenesis. Furthermore, the fruit fly has been extensively utilized to investigate neural biology and neurodevelopment, enabling key insights into human neurological disorders such as Alzheimer's and Parkinson's diseases. \n",
        "\n",
        "The close evolutionary relationship between *D. melanogaster* and humans has also enabled the former's utility for modeling early human migrations. Through these studies, researchers have gained critical understanding of local adaptation as human populations expanded out of Africa.\n",
        "\n",
        "Moreover, the fruit fly represented one of the first animal models for comprehensive investigation of genotype-phenotype linkages across the whole genome. In 2012, the Drosophila Genetics Reference Panel (DGRP) project fully sequenced and released nearly 200 \"reference\" fly lines, along with matched phenotype data. The DGRP serves as a public resource to enable diverse genome-phenotype association studies, with all data freely available online (http://dgrp.gnets.ncsu.edu/).\n",
        "\n",
        "In this research, we develop a neural network approach to model the genotype-phenotype map between genomic variation and lifespan in fruit flies. Specifically, we aim to predict \"longevity\" phenotypes from >17,000 single nucleotide polymorphism (SNP) genotypes assayed across the autosomal chromosomes of 182 Drosophila lines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBTJ3QsoARIc"
      },
      "source": [
        "## Data Curation and Preprocessing\n",
        "\n",
        "The SNP genotype and longevity phenotype data for 182 Drosophila lines were obtained from the publicly available DGRP resource. For this project, we utilized a subset of these data files provided in comma-separated values (.csv) format. \n",
        "\n",
        "The .csv file structure enables simple storage of the genome-wide SNP profiles, with each row representing a fly line. The SNPs are encoded as binary values in adjacent columns. Additional columns capture the unique line identifier (\"SID\") and experimentally measured longevity score (\"LS\") for each genotype. \n",
        "\n",
        "CSV files are widely used for storing biological data and can be easily imported into programming environments like Python and R for analysis. We leverage this capability to ingest the data and convert to formats amenable for neural network training, without directly manipulating the raw genotype values.\n",
        "\n",
        "Specifically, we import the remote .csv file into a pandas DataFrame using the URL:\n",
        "\n",
        "https://raw.githubusercontent.com/bryankolaczkowski/ALS3200C/main/phenopred.data.csv\n",
        "\n",
        "This encompasses all 182 lines, with SNPs in 17,165 columns and the \"LS\" phenotype values. We then sample and split this DataFrame into disjoint training and validation sets for model fitting and evaluation.\n",
        "\n",
        "While data preprocessing steps like imputation and normalization can be beneficial for genome-wide association studies, we avoid any direct manipulation of the genotypes themselves to preclude inadvertently biasing later analysis. However, conversion from DataFrames to NumPy arrays and TensorFlow Datasets does facilitate model implementation in the following sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "x09eqzW7ivpB",
        "outputId": "1581fe1a-4dda-4fac-fef8-df88e354ed4d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SID</th>\n",
              "      <th>SNP0</th>\n",
              "      <th>SNP1</th>\n",
              "      <th>SNP2</th>\n",
              "      <th>SNP3</th>\n",
              "      <th>SNP4</th>\n",
              "      <th>SNP5</th>\n",
              "      <th>SNP6</th>\n",
              "      <th>SNP7</th>\n",
              "      <th>SNP8</th>\n",
              "      <th>...</th>\n",
              "      <th>SNP17156</th>\n",
              "      <th>SNP17157</th>\n",
              "      <th>SNP17158</th>\n",
              "      <th>SNP17159</th>\n",
              "      <th>SNP17160</th>\n",
              "      <th>SNP17161</th>\n",
              "      <th>SNP17162</th>\n",
              "      <th>SNP17163</th>\n",
              "      <th>SNP17164</th>\n",
              "      <th>LS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>S0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>46.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>S1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>22.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>S2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>45.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>S3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>34.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>S4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39.15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 17167 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  SID  SNP0  SNP1  SNP2  SNP3  SNP4  SNP5  SNP6  SNP7  SNP8  ...  SNP17156   \n",
              "0  S0     0     0     0     1     1     0     1     1     1  ...         0  \\\n",
              "1  S1     0     0     0     0     0     0     0     0     0  ...         0   \n",
              "2  S2     0     0     0     0     0     0     1     1     0  ...         0   \n",
              "3  S3     0     0     0     1     1     0     1     1     0  ...         0   \n",
              "4  S4     0     0     0     0     0     0     1     0     0  ...         0   \n",
              "\n",
              "   SNP17157  SNP17158  SNP17159  SNP17160  SNP17161  SNP17162  SNP17163   \n",
              "0         0         0         0         0         1         0         0  \\\n",
              "1         0         0         0         0         1         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   SNP17164     LS  \n",
              "0         0  46.83  \n",
              "1         0  22.67  \n",
              "2         0  45.55  \n",
              "3         0  34.45  \n",
              "4         0  39.15  \n",
              "\n",
              "[5 rows x 17167 columns]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas\n",
        "dataframe = pandas.read_csv('https://raw.githubusercontent.com/bryankolaczkowski/ALS3200C/main/phenopred.data.csv')\n",
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D81y9G7jmx0"
      },
      "source": [
        "the .csv file structure enables simple storage of the genome-wide SNP profiles, with each row representing a fly line. The SNPs are encoded as binary values in adjacent columns. Additional columns capture the unique line identifier (\"SID\") and experimentally measured longevity score (\"LS\") for each genotype.\n",
        "\n",
        "The pandas library imports the data into a DataFrame structure, automatically assigning incremental row numbers and applying the original column labels from the file. This maintains the fly line identifier and phenotype data alongside the genotypes in a convenient format amenable for analysis.\n",
        "\n",
        "Critically, the SNP identifiers are consistent across all lines, with the same genomic positional variant encoded in the same ordinal column. This allows allele frequencies and genotypes to be compared at single base pair resolution across the 182 sequenced Drosophila genomes.\n",
        "\n",
        "The raw SNPs are encoded as binary values of 0 or 1 per column, representing the simplest possible genotype encoding. A value of 0 denotes the reference allele, meaning the line matches the canonical FlyBase genome at that location. A 1 denotes an alternative allele, indicating divergence from the reference.\n",
        "\n",
        "Notably, the fly lines were intentionally inbred to full homozygosity, meaning they carry two identical alleles (0 or 1) at all sites. This simplifies analysis by avoiding heterozygous genotypes. However, it may reduce applicability of the panel to natural populations.\n",
        "\n",
        "Conceptually, the 17,165 SNPs assayed across two chromosomes capture genomic variation at an extremely high density. This enables sensitive genome-wide association mapping of phenotypes like longevity.\n",
        "\n",
        "By importing the data into a DataFrame and converting to arrays, we have full access to the genotypes and phenotypes for modeling, while ensuring the integrity of the original data is preserved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aikV1kgdkrJJ",
        "outputId": "5ed17505-0c4e-4827-eaa1-2674ffe9e1ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(182, 17167)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataframe.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmNAHcchKI3P"
      },
      "source": [
        "### end-to-end data engineering pipeline\n",
        "The full data preprocessing workflow can be consolidated into the following pipeline:\n",
        "\n",
        "1. Import the remote CSV file into a pandas DataFrame\n",
        "2. Split the DataFrame into complementary training and validation subsets\n",
        "3. Isolate the SNP identifiers and longevity scores into separate NumPy arrays\n",
        "4. Convert the x (genotypes) and y (phenotypes) arrays into TensorFlow Dataset objects\n",
        "5. Batch the Datasets for efficient iteration during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kejKFvgDKWrt",
        "outputId": "d87de0ba-ec5a-4366-8e95-813582417d7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(182, 17167)\n",
            "(146, 17167) (36, 17167) (182, 17167)\n",
            "(146, 17165) (36, 17165)\n",
            "(146,) (36,)\n"
          ]
        }
      ],
      "source": [
        "import pandas\n",
        "import tensorflow as tf\n",
        "\n",
        "# read data file and confirm data shape\n",
        "dataframe = pandas.read_csv('https://raw.githubusercontent.com/bryankolaczkowski/ALS3200C/main/phenopred.data.csv')\n",
        "print(dataframe.shape)\n",
        "\n",
        "# split into training and validation subsets, and confirm shape\n",
        "train_dataframe = dataframe.sample(frac=0.8, random_state=402201)\n",
        "valid_dataframe = dataframe.drop(train_dataframe.index)\n",
        "print(train_dataframe.shape, valid_dataframe.shape, dataframe.shape)\n",
        "\n",
        "# extract explanatory variables, convert to numpy and confirm shapes\n",
        "snp_ids = [ x for x in dataframe.columns if x.find('SNP') == 0 ]\n",
        "train_x = train_dataframe[snp_ids].to_numpy()\n",
        "valid_x = valid_dataframe[snp_ids].to_numpy()\n",
        "print(train_x.shape, valid_x.shape)\n",
        "\n",
        "# extract response variables, convert to numpy and confirm shapes\n",
        "train_y = train_dataframe['LS'].to_numpy()\n",
        "valid_y = valid_dataframe['LS'].to_numpy()\n",
        "print(train_y.shape, valid_y.shape)\n",
        "\n",
        "# package into tensorflow.Dataset objects and batch\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_x,train_y)).batch(10)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x,valid_y)).batch(36)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SGjLT_BPoAEH",
        "outputId": "10b86d89-a1d2-4a20-f6bc-20eaafc60c48"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(182, 17167)\n",
            "(146, 17167) (36, 17167) (182, 17167)\n",
            "(146, 17165) (36, 17165)\n",
            "(146,) (36,)\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 1)                 17166     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17166 (67.05 KB)\n",
            "Trainable params: 17166 (67.05 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 18.2662 - val_loss: 7.5142\n",
            "Epoch 2/500\n",
            "15/15 [==============================] - 0s 937us/step - loss: 6.9236 - val_loss: 6.3842\n",
            "Epoch 3/500\n",
            "15/15 [==============================] - 0s 854us/step - loss: 4.0679 - val_loss: 4.4789\n",
            "Epoch 4/500\n",
            "15/15 [==============================] - 0s 874us/step - loss: 2.9096 - val_loss: 4.3724\n",
            "Epoch 5/500\n",
            "15/15 [==============================] - 0s 900us/step - loss: 2.5906 - val_loss: 4.2475\n",
            "Epoch 6/500\n",
            "15/15 [==============================] - 0s 847us/step - loss: 1.7589 - val_loss: 4.3379\n",
            "Epoch 7/500\n",
            "15/15 [==============================] - 0s 836us/step - loss: 2.0347 - val_loss: 4.9218\n",
            "Epoch 8/500\n",
            "15/15 [==============================] - 0s 831us/step - loss: 2.4140 - val_loss: 5.5462\n",
            "Epoch 9/500\n",
            "15/15 [==============================] - 0s 887us/step - loss: 4.4071 - val_loss: 4.8043\n",
            "Epoch 10/500\n",
            "15/15 [==============================] - 0s 800us/step - loss: 3.1831 - val_loss: 4.5503\n",
            "Epoch 11/500\n",
            "15/15 [==============================] - 0s 866us/step - loss: 2.3519 - val_loss: 4.4691\n",
            "Epoch 12/500\n",
            "15/15 [==============================] - 0s 835us/step - loss: 1.6762 - val_loss: 4.4648\n",
            "Epoch 13/500\n",
            "15/15 [==============================] - 0s 901us/step - loss: 1.8306 - val_loss: 4.5526\n",
            "Epoch 14/500\n",
            "15/15 [==============================] - 0s 804us/step - loss: 3.3002 - val_loss: 4.6982\n",
            "Epoch 15/500\n",
            "15/15 [==============================] - 0s 824us/step - loss: 3.5791 - val_loss: 4.5595\n",
            "Epoch 16/500\n",
            "15/15 [==============================] - 0s 915us/step - loss: 2.3936 - val_loss: 4.1834\n",
            "Epoch 17/500\n",
            "15/15 [==============================] - 0s 877us/step - loss: 1.4825 - val_loss: 4.0746\n",
            "Epoch 18/500\n",
            "15/15 [==============================] - 0s 897us/step - loss: 0.9349 - val_loss: 4.2037\n",
            "Epoch 19/500\n",
            "15/15 [==============================] - 0s 844us/step - loss: 1.0068 - val_loss: 4.9041\n",
            "Epoch 20/500\n",
            "15/15 [==============================] - 0s 949us/step - loss: 2.9149 - val_loss: 4.1729\n",
            "Epoch 21/500\n",
            "15/15 [==============================] - 0s 894us/step - loss: 2.7903 - val_loss: 4.5994\n",
            "Epoch 22/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.6667 - val_loss: 4.9438\n",
            "Epoch 23/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 2.9587 - val_loss: 4.2286\n",
            "Epoch 24/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.3682 - val_loss: 4.2514\n",
            "Epoch 25/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.6257 - val_loss: 5.1120\n",
            "Epoch 26/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.2273 - val_loss: 4.8384\n",
            "Epoch 27/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.1881 - val_loss: 4.1977\n",
            "Epoch 28/500\n",
            "15/15 [==============================] - 0s 980us/step - loss: 2.6312 - val_loss: 4.7477\n",
            "Epoch 29/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.0456 - val_loss: 4.3970\n",
            "Epoch 30/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 1.4289 - val_loss: 4.5364\n",
            "Epoch 31/500\n",
            "15/15 [==============================] - 0s 981us/step - loss: 1.7731 - val_loss: 4.2952\n",
            "Epoch 32/500\n",
            "15/15 [==============================] - 0s 904us/step - loss: 3.0909 - val_loss: 5.0006\n",
            "Epoch 33/500\n",
            "15/15 [==============================] - 0s 920us/step - loss: 2.9632 - val_loss: 4.8205\n",
            "Epoch 34/500\n",
            "15/15 [==============================] - 0s 929us/step - loss: 2.8128 - val_loss: 4.1084\n",
            "Epoch 35/500\n",
            "15/15 [==============================] - 0s 884us/step - loss: 1.8796 - val_loss: 4.0079\n",
            "Epoch 36/500\n",
            "15/15 [==============================] - 0s 877us/step - loss: 1.6280 - val_loss: 3.9717\n",
            "Epoch 37/500\n",
            "15/15 [==============================] - 0s 899us/step - loss: 1.5573 - val_loss: 4.5711\n",
            "Epoch 38/500\n",
            "15/15 [==============================] - 0s 860us/step - loss: 2.6189 - val_loss: 4.7755\n",
            "Epoch 39/500\n",
            "15/15 [==============================] - 0s 897us/step - loss: 4.0720 - val_loss: 4.2081\n",
            "Epoch 40/500\n",
            "15/15 [==============================] - 0s 853us/step - loss: 1.8136 - val_loss: 4.1385\n",
            "Epoch 41/500\n",
            "15/15 [==============================] - 0s 829us/step - loss: 1.4248 - val_loss: 4.3082\n",
            "Epoch 42/500\n",
            "15/15 [==============================] - 0s 863us/step - loss: 1.2374 - val_loss: 4.0741\n",
            "Epoch 43/500\n",
            "15/15 [==============================] - 0s 863us/step - loss: 1.9391 - val_loss: 5.2680\n",
            "Epoch 44/500\n",
            "15/15 [==============================] - 0s 867us/step - loss: 3.6233 - val_loss: 4.1094\n",
            "Epoch 45/500\n",
            "15/15 [==============================] - 0s 980us/step - loss: 3.3769 - val_loss: 4.0479\n",
            "Epoch 46/500\n",
            "15/15 [==============================] - 0s 858us/step - loss: 1.5398 - val_loss: 3.9858\n",
            "Epoch 47/500\n",
            "15/15 [==============================] - 0s 839us/step - loss: 0.8988 - val_loss: 3.9453\n",
            "Epoch 48/500\n",
            "15/15 [==============================] - 0s 791us/step - loss: 0.5485 - val_loss: 4.0295\n",
            "Epoch 49/500\n",
            "15/15 [==============================] - 0s 965us/step - loss: 0.6128 - val_loss: 4.1248\n",
            "Epoch 50/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.7689 - val_loss: 4.0880\n",
            "Epoch 51/500\n",
            "15/15 [==============================] - 0s 893us/step - loss: 0.7938 - val_loss: 4.0467\n",
            "Epoch 52/500\n",
            "15/15 [==============================] - 0s 947us/step - loss: 1.4815 - val_loss: 4.4059\n",
            "Epoch 53/500\n",
            "15/15 [==============================] - 0s 892us/step - loss: 3.6163 - val_loss: 4.2947\n",
            "Epoch 54/500\n",
            "15/15 [==============================] - 0s 961us/step - loss: 3.7847 - val_loss: 4.0801\n",
            "Epoch 55/500\n",
            "15/15 [==============================] - 0s 892us/step - loss: 1.8941 - val_loss: 4.0056\n",
            "Epoch 56/500\n",
            "15/15 [==============================] - 0s 970us/step - loss: 1.4737 - val_loss: 4.1334\n",
            "Epoch 57/500\n",
            "15/15 [==============================] - 0s 995us/step - loss: 0.7793 - val_loss: 4.0272\n",
            "Epoch 58/500\n",
            "15/15 [==============================] - 0s 915us/step - loss: 0.8437 - val_loss: 4.3473\n",
            "Epoch 59/500\n",
            "15/15 [==============================] - 0s 931us/step - loss: 0.8415 - val_loss: 4.4644\n",
            "Epoch 60/500\n",
            "15/15 [==============================] - 0s 915us/step - loss: 1.8425 - val_loss: 4.0454\n",
            "Epoch 61/500\n",
            "15/15 [==============================] - 0s 932us/step - loss: 3.3931 - val_loss: 5.4405\n",
            "Epoch 62/500\n",
            "15/15 [==============================] - 0s 861us/step - loss: 3.7977 - val_loss: 4.0292\n",
            "Epoch 63/500\n",
            "15/15 [==============================] - 0s 986us/step - loss: 1.9294 - val_loss: 4.0715\n",
            "Epoch 64/500\n",
            "15/15 [==============================] - 0s 887us/step - loss: 1.4652 - val_loss: 4.2335\n",
            "Epoch 65/500\n",
            "15/15 [==============================] - 0s 886us/step - loss: 1.1057 - val_loss: 4.7442\n",
            "Epoch 66/500\n",
            "15/15 [==============================] - 0s 824us/step - loss: 1.8876 - val_loss: 4.5788\n",
            "Epoch 67/500\n",
            "15/15 [==============================] - 0s 868us/step - loss: 4.0995 - val_loss: 5.4009\n",
            "Epoch 68/500\n",
            "15/15 [==============================] - 0s 863us/step - loss: 2.6116 - val_loss: 4.0765\n",
            "Epoch 69/500\n",
            "15/15 [==============================] - 0s 913us/step - loss: 1.7471 - val_loss: 4.1604\n",
            "Epoch 70/500\n",
            "15/15 [==============================] - 0s 851us/step - loss: 1.1295 - val_loss: 4.4190\n",
            "Epoch 71/500\n",
            "15/15 [==============================] - 0s 868us/step - loss: 1.7262 - val_loss: 4.8811\n",
            "Epoch 72/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.7445 - val_loss: 5.2615\n",
            "Epoch 73/500\n",
            "15/15 [==============================] - 0s 897us/step - loss: 2.9366 - val_loss: 4.1623\n",
            "Epoch 74/500\n",
            "15/15 [==============================] - 0s 805us/step - loss: 1.4689 - val_loss: 4.0041\n",
            "Epoch 75/500\n",
            "15/15 [==============================] - 0s 864us/step - loss: 0.7203 - val_loss: 4.1359\n",
            "Epoch 76/500\n",
            "15/15 [==============================] - 0s 834us/step - loss: 0.5843 - val_loss: 4.0916\n",
            "Epoch 77/500\n",
            "15/15 [==============================] - 0s 862us/step - loss: 1.3266 - val_loss: 4.6293\n",
            "Epoch 78/500\n",
            "15/15 [==============================] - 0s 852us/step - loss: 2.2175 - val_loss: 4.0930\n",
            "Epoch 79/500\n",
            "15/15 [==============================] - 0s 802us/step - loss: 3.3343 - val_loss: 4.8940\n",
            "Epoch 80/500\n",
            "15/15 [==============================] - 0s 824us/step - loss: 2.7073 - val_loss: 4.4897\n",
            "Epoch 81/500\n",
            "15/15 [==============================] - 0s 869us/step - loss: 1.8953 - val_loss: 4.5178\n",
            "Epoch 82/500\n",
            "15/15 [==============================] - 0s 853us/step - loss: 1.4898 - val_loss: 4.5584\n",
            "Epoch 83/500\n",
            "15/15 [==============================] - 0s 811us/step - loss: 2.6014 - val_loss: 4.9068\n",
            "Epoch 84/500\n",
            "15/15 [==============================] - 0s 845us/step - loss: 3.2139 - val_loss: 4.4533\n",
            "Epoch 85/500\n",
            "15/15 [==============================] - 0s 833us/step - loss: 2.7581 - val_loss: 3.9281\n",
            "Epoch 86/500\n",
            "15/15 [==============================] - 0s 838us/step - loss: 1.8189 - val_loss: 3.8315\n",
            "Epoch 87/500\n",
            "15/15 [==============================] - 0s 877us/step - loss: 1.7015 - val_loss: 4.0338\n",
            "Epoch 88/500\n",
            "15/15 [==============================] - 0s 918us/step - loss: 1.7479 - val_loss: 4.6047\n",
            "Epoch 89/500\n",
            "15/15 [==============================] - 0s 916us/step - loss: 2.4921 - val_loss: 4.4800\n",
            "Epoch 90/500\n",
            "15/15 [==============================] - 0s 963us/step - loss: 3.8503 - val_loss: 4.8633\n",
            "Epoch 91/500\n",
            "15/15 [==============================] - 0s 900us/step - loss: 1.9918 - val_loss: 4.2004\n",
            "Epoch 92/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.2761 - val_loss: 4.0433\n",
            "Epoch 93/500\n",
            "15/15 [==============================] - 0s 766us/step - loss: 0.8059 - val_loss: 4.1587\n",
            "Epoch 94/500\n",
            "15/15 [==============================] - 0s 886us/step - loss: 1.1616 - val_loss: 4.6759\n",
            "Epoch 95/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.3661 - val_loss: 4.7294\n",
            "Epoch 96/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.0299 - val_loss: 4.9680\n",
            "Epoch 97/500\n",
            "15/15 [==============================] - 0s 948us/step - loss: 2.4761 - val_loss: 4.0492\n",
            "Epoch 98/500\n",
            "15/15 [==============================] - 0s 811us/step - loss: 1.4548 - val_loss: 3.9090\n",
            "Epoch 99/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.7056 - val_loss: 3.9979\n",
            "Epoch 100/500\n",
            "15/15 [==============================] - 0s 843us/step - loss: 1.1412 - val_loss: 3.9850\n",
            "Epoch 101/500\n",
            "15/15 [==============================] - 0s 825us/step - loss: 0.7096 - val_loss: 3.9208\n",
            "Epoch 102/500\n",
            "15/15 [==============================] - 0s 851us/step - loss: 0.9859 - val_loss: 4.9808\n",
            "Epoch 103/500\n",
            "15/15 [==============================] - 0s 818us/step - loss: 3.5916 - val_loss: 4.0325\n",
            "Epoch 104/500\n",
            "15/15 [==============================] - 0s 772us/step - loss: 3.1303 - val_loss: 4.1185\n",
            "Epoch 105/500\n",
            "15/15 [==============================] - 0s 829us/step - loss: 2.1291 - val_loss: 4.0175\n",
            "Epoch 106/500\n",
            "15/15 [==============================] - 0s 845us/step - loss: 1.0434 - val_loss: 4.0302\n",
            "Epoch 107/500\n",
            "15/15 [==============================] - 0s 838us/step - loss: 0.8399 - val_loss: 4.2313\n",
            "Epoch 108/500\n",
            "15/15 [==============================] - 0s 842us/step - loss: 1.0886 - val_loss: 4.3194\n",
            "Epoch 109/500\n",
            "15/15 [==============================] - 0s 817us/step - loss: 1.2334 - val_loss: 5.8495\n",
            "Epoch 110/500\n",
            "15/15 [==============================] - 0s 838us/step - loss: 3.8781 - val_loss: 4.7909\n",
            "Epoch 111/500\n",
            "15/15 [==============================] - 0s 826us/step - loss: 3.1804 - val_loss: 4.4063\n",
            "Epoch 112/500\n",
            "15/15 [==============================] - 0s 772us/step - loss: 2.1034 - val_loss: 4.2661\n",
            "Epoch 113/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 1.1506 - val_loss: 3.8738\n",
            "Epoch 114/500\n",
            "15/15 [==============================] - 0s 829us/step - loss: 0.7025 - val_loss: 4.0949\n",
            "Epoch 115/500\n",
            "15/15 [==============================] - 0s 787us/step - loss: 1.2392 - val_loss: 4.6267\n",
            "Epoch 116/500\n",
            "15/15 [==============================] - 0s 803us/step - loss: 2.8970 - val_loss: 3.8793\n",
            "Epoch 117/500\n",
            "15/15 [==============================] - 0s 739us/step - loss: 3.3384 - val_loss: 5.0220\n",
            "Epoch 118/500\n",
            "15/15 [==============================] - 0s 870us/step - loss: 2.3553 - val_loss: 4.1556\n",
            "Epoch 119/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.1542 - val_loss: 4.0900\n",
            "Epoch 120/500\n",
            "15/15 [==============================] - 0s 882us/step - loss: 0.7578 - val_loss: 4.2649\n",
            "Epoch 121/500\n",
            "15/15 [==============================] - 0s 872us/step - loss: 0.6977 - val_loss: 4.1296\n",
            "Epoch 122/500\n",
            "15/15 [==============================] - 0s 860us/step - loss: 0.6570 - val_loss: 4.3427\n",
            "Epoch 123/500\n",
            "15/15 [==============================] - 0s 866us/step - loss: 1.2578 - val_loss: 4.9457\n",
            "Epoch 124/500\n",
            "15/15 [==============================] - 0s 797us/step - loss: 3.5572 - val_loss: 5.1861\n",
            "Epoch 125/500\n",
            "15/15 [==============================] - 0s 831us/step - loss: 2.9497 - val_loss: 4.2558\n",
            "Epoch 126/500\n",
            "15/15 [==============================] - 0s 864us/step - loss: 1.9457 - val_loss: 4.3294\n",
            "Epoch 127/500\n",
            "15/15 [==============================] - 0s 838us/step - loss: 1.5964 - val_loss: 4.5113\n",
            "Epoch 128/500\n",
            "15/15 [==============================] - 0s 860us/step - loss: 1.9052 - val_loss: 4.4601\n",
            "Epoch 129/500\n",
            "15/15 [==============================] - 0s 768us/step - loss: 3.2156 - val_loss: 5.4015\n",
            "Epoch 130/500\n",
            "15/15 [==============================] - 0s 843us/step - loss: 2.9141 - val_loss: 4.6912\n",
            "Epoch 131/500\n",
            "15/15 [==============================] - 0s 906us/step - loss: 1.9995 - val_loss: 4.4951\n",
            "Epoch 132/500\n",
            "15/15 [==============================] - 0s 919us/step - loss: 1.3528 - val_loss: 4.4165\n",
            "Epoch 133/500\n",
            "15/15 [==============================] - 0s 929us/step - loss: 1.9111 - val_loss: 3.9039\n",
            "Epoch 134/500\n",
            "15/15 [==============================] - 0s 941us/step - loss: 2.3016 - val_loss: 4.5048\n",
            "Epoch 135/500\n",
            "15/15 [==============================] - 0s 941us/step - loss: 3.3485 - val_loss: 4.1250\n",
            "Epoch 136/500\n",
            "15/15 [==============================] - 0s 931us/step - loss: 2.7023 - val_loss: 4.0080\n",
            "Epoch 137/500\n",
            "15/15 [==============================] - 0s 947us/step - loss: 1.5669 - val_loss: 4.1102\n",
            "Epoch 138/500\n",
            "15/15 [==============================] - 0s 959us/step - loss: 0.7482 - val_loss: 4.0031\n",
            "Epoch 139/500\n",
            "15/15 [==============================] - 0s 935us/step - loss: 1.1411 - val_loss: 4.1884\n",
            "Epoch 140/500\n",
            "15/15 [==============================] - 0s 884us/step - loss: 1.3535 - val_loss: 4.0862\n",
            "Epoch 141/500\n",
            "15/15 [==============================] - 0s 929us/step - loss: 2.2343 - val_loss: 5.5157\n",
            "Epoch 142/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.8028 - val_loss: 3.9960\n",
            "Epoch 143/500\n",
            "15/15 [==============================] - 0s 950us/step - loss: 2.5117 - val_loss: 3.9632\n",
            "Epoch 144/500\n",
            "15/15 [==============================] - 0s 935us/step - loss: 1.1831 - val_loss: 4.1044\n",
            "Epoch 145/500\n",
            "15/15 [==============================] - 0s 908us/step - loss: 0.9091 - val_loss: 3.9532\n",
            "Epoch 146/500\n",
            "15/15 [==============================] - 0s 848us/step - loss: 1.2552 - val_loss: 3.9783\n",
            "Epoch 147/500\n",
            "15/15 [==============================] - 0s 784us/step - loss: 1.0404 - val_loss: 3.9567\n",
            "Epoch 148/500\n",
            "15/15 [==============================] - 0s 808us/step - loss: 2.0394 - val_loss: 5.1776\n",
            "Epoch 149/500\n",
            "15/15 [==============================] - 0s 861us/step - loss: 4.2430 - val_loss: 4.4872\n",
            "Epoch 150/500\n",
            "15/15 [==============================] - 0s 883us/step - loss: 2.4297 - val_loss: 3.9302\n",
            "Epoch 151/500\n",
            "15/15 [==============================] - 0s 898us/step - loss: 1.3233 - val_loss: 4.2589\n",
            "Epoch 152/500\n",
            "15/15 [==============================] - 0s 926us/step - loss: 1.3235 - val_loss: 4.0416\n",
            "Epoch 153/500\n",
            "15/15 [==============================] - 0s 953us/step - loss: 1.0442 - val_loss: 4.0331\n",
            "Epoch 154/500\n",
            "15/15 [==============================] - 0s 961us/step - loss: 1.7927 - val_loss: 5.0337\n",
            "Epoch 155/500\n",
            "15/15 [==============================] - 0s 889us/step - loss: 3.2218 - val_loss: 4.2570\n",
            "Epoch 156/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.2421 - val_loss: 4.2443\n",
            "Epoch 157/500\n",
            "15/15 [==============================] - 0s 938us/step - loss: 2.2828 - val_loss: 4.1680\n",
            "Epoch 158/500\n",
            "15/15 [==============================] - 0s 912us/step - loss: 1.4890 - val_loss: 4.0256\n",
            "Epoch 159/500\n",
            "15/15 [==============================] - 0s 891us/step - loss: 1.2108 - val_loss: 4.2982\n",
            "Epoch 160/500\n",
            "15/15 [==============================] - 0s 889us/step - loss: 1.5575 - val_loss: 4.7611\n",
            "Epoch 161/500\n",
            "15/15 [==============================] - 0s 844us/step - loss: 3.3552 - val_loss: 4.6807\n",
            "Epoch 162/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.0287 - val_loss: 4.1384\n",
            "Epoch 163/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.0377 - val_loss: 4.2122\n",
            "Epoch 164/500\n",
            "15/15 [==============================] - 0s 829us/step - loss: 1.2153 - val_loss: 4.2549\n",
            "Epoch 165/500\n",
            "15/15 [==============================] - 0s 819us/step - loss: 1.4455 - val_loss: 5.8970\n",
            "Epoch 166/500\n",
            "15/15 [==============================] - 0s 913us/step - loss: 3.6246 - val_loss: 4.0586\n",
            "Epoch 167/500\n",
            "15/15 [==============================] - 0s 947us/step - loss: 3.0826 - val_loss: 4.3119\n",
            "Epoch 168/500\n",
            "15/15 [==============================] - 0s 858us/step - loss: 1.9007 - val_loss: 4.0614\n",
            "Epoch 169/500\n",
            "15/15 [==============================] - 0s 891us/step - loss: 1.3047 - val_loss: 4.0518\n",
            "Epoch 170/500\n",
            "15/15 [==============================] - 0s 926us/step - loss: 0.9630 - val_loss: 4.6174\n",
            "Epoch 171/500\n",
            "15/15 [==============================] - 0s 850us/step - loss: 2.7526 - val_loss: 3.8897\n",
            "Epoch 172/500\n",
            "15/15 [==============================] - 0s 812us/step - loss: 3.6889 - val_loss: 4.8361\n",
            "Epoch 173/500\n",
            "15/15 [==============================] - 0s 824us/step - loss: 2.3120 - val_loss: 3.8965\n",
            "Epoch 174/500\n",
            "15/15 [==============================] - 0s 845us/step - loss: 1.6413 - val_loss: 4.0054\n",
            "Epoch 175/500\n",
            "15/15 [==============================] - 0s 883us/step - loss: 1.0128 - val_loss: 4.2045\n",
            "Epoch 176/500\n",
            "15/15 [==============================] - 0s 950us/step - loss: 1.2763 - val_loss: 5.1677\n",
            "Epoch 177/500\n",
            "15/15 [==============================] - 0s 922us/step - loss: 3.2625 - val_loss: 4.2171\n",
            "Epoch 178/500\n",
            "15/15 [==============================] - 0s 941us/step - loss: 3.2167 - val_loss: 4.5577\n",
            "Epoch 179/500\n",
            "15/15 [==============================] - 0s 948us/step - loss: 1.6784 - val_loss: 4.0099\n",
            "Epoch 180/500\n",
            "15/15 [==============================] - 0s 917us/step - loss: 0.7917 - val_loss: 3.9681\n",
            "Epoch 181/500\n",
            "15/15 [==============================] - 0s 876us/step - loss: 0.6325 - val_loss: 3.9229\n",
            "Epoch 182/500\n",
            "15/15 [==============================] - 0s 893us/step - loss: 0.8664 - val_loss: 4.1317\n",
            "Epoch 183/500\n",
            "15/15 [==============================] - 0s 853us/step - loss: 1.4170 - val_loss: 5.1297\n",
            "Epoch 184/500\n",
            "15/15 [==============================] - 0s 858us/step - loss: 3.9585 - val_loss: 4.2082\n",
            "Epoch 185/500\n",
            "15/15 [==============================] - 0s 841us/step - loss: 3.0993 - val_loss: 4.0804\n",
            "Epoch 186/500\n",
            "15/15 [==============================] - 0s 806us/step - loss: 2.0718 - val_loss: 3.9699\n",
            "Epoch 187/500\n",
            "15/15 [==============================] - 0s 845us/step - loss: 1.4147 - val_loss: 4.1348\n",
            "Epoch 188/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.0469 - val_loss: 4.2418\n",
            "Epoch 189/500\n",
            "15/15 [==============================] - 0s 876us/step - loss: 1.2315 - val_loss: 4.9182\n",
            "Epoch 190/500\n",
            "15/15 [==============================] - 0s 880us/step - loss: 3.0268 - val_loss: 4.1403\n",
            "Epoch 191/500\n",
            "15/15 [==============================] - 0s 796us/step - loss: 3.5225 - val_loss: 4.5433\n",
            "Epoch 192/500\n",
            "15/15 [==============================] - 0s 803us/step - loss: 1.9742 - val_loss: 3.9455\n",
            "Epoch 193/500\n",
            "15/15 [==============================] - 0s 832us/step - loss: 1.2709 - val_loss: 3.8980\n",
            "Epoch 194/500\n",
            "15/15 [==============================] - 0s 858us/step - loss: 0.8585 - val_loss: 3.9960\n",
            "Epoch 195/500\n",
            "15/15 [==============================] - 0s 836us/step - loss: 1.3158 - val_loss: 4.5282\n",
            "Epoch 196/500\n",
            "15/15 [==============================] - 0s 810us/step - loss: 2.8490 - val_loss: 3.9567\n",
            "Epoch 197/500\n",
            "15/15 [==============================] - 0s 905us/step - loss: 3.3344 - val_loss: 4.7582\n",
            "Epoch 198/500\n",
            "15/15 [==============================] - 0s 833us/step - loss: 2.4425 - val_loss: 4.5030\n",
            "Epoch 199/500\n",
            "15/15 [==============================] - 0s 690us/step - loss: 1.9290 - val_loss: 4.7402\n",
            "Epoch 200/500\n",
            "15/15 [==============================] - 0s 665us/step - loss: 1.7833 - val_loss: 4.4535\n",
            "Epoch 201/500\n",
            "15/15 [==============================] - 0s 918us/step - loss: 2.3722 - val_loss: 3.9361\n",
            "Epoch 202/500\n",
            "15/15 [==============================] - 0s 980us/step - loss: 2.5843 - val_loss: 4.4725\n",
            "Epoch 203/500\n",
            "15/15 [==============================] - 0s 816us/step - loss: 2.0190 - val_loss: 4.5379\n",
            "Epoch 204/500\n",
            "15/15 [==============================] - 0s 835us/step - loss: 2.2092 - val_loss: 4.3392\n",
            "Epoch 205/500\n",
            "15/15 [==============================] - 0s 808us/step - loss: 2.9318 - val_loss: 4.6503\n",
            "Epoch 206/500\n",
            "15/15 [==============================] - 0s 837us/step - loss: 2.0367 - val_loss: 4.6118\n",
            "Epoch 207/500\n",
            "15/15 [==============================] - 0s 817us/step - loss: 2.0606 - val_loss: 4.9797\n",
            "Epoch 208/500\n",
            "15/15 [==============================] - 0s 800us/step - loss: 2.6339 - val_loss: 4.0133\n",
            "Epoch 209/500\n",
            "15/15 [==============================] - 0s 827us/step - loss: 2.3469 - val_loss: 4.4861\n",
            "Epoch 210/500\n",
            "15/15 [==============================] - 0s 828us/step - loss: 2.0930 - val_loss: 4.3271\n",
            "Epoch 211/500\n",
            "15/15 [==============================] - 0s 854us/step - loss: 1.7704 - val_loss: 4.8698\n",
            "Epoch 212/500\n",
            "15/15 [==============================] - 0s 852us/step - loss: 3.0083 - val_loss: 3.9756\n",
            "Epoch 213/500\n",
            "15/15 [==============================] - 0s 910us/step - loss: 2.7587 - val_loss: 4.8650\n",
            "Epoch 214/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 2.2348 - val_loss: 4.3233\n",
            "Epoch 215/500\n",
            "15/15 [==============================] - 0s 952us/step - loss: 1.5103 - val_loss: 4.4079\n",
            "Epoch 216/500\n",
            "15/15 [==============================] - 0s 942us/step - loss: 1.3423 - val_loss: 4.3869\n",
            "Epoch 217/500\n",
            "15/15 [==============================] - 0s 899us/step - loss: 2.5852 - val_loss: 5.1149\n",
            "Epoch 218/500\n",
            "15/15 [==============================] - 0s 912us/step - loss: 3.4343 - val_loss: 4.1106\n",
            "Epoch 219/500\n",
            "15/15 [==============================] - 0s 915us/step - loss: 2.0235 - val_loss: 4.2712\n",
            "Epoch 220/500\n",
            "15/15 [==============================] - 0s 871us/step - loss: 1.4412 - val_loss: 4.3605\n",
            "Epoch 221/500\n",
            "15/15 [==============================] - 0s 874us/step - loss: 0.9375 - val_loss: 4.3772\n",
            "Epoch 222/500\n",
            "15/15 [==============================] - 0s 852us/step - loss: 1.7345 - val_loss: 4.0442\n",
            "Epoch 223/500\n",
            "15/15 [==============================] - 0s 859us/step - loss: 2.8783 - val_loss: 5.5365\n",
            "Epoch 224/500\n",
            "15/15 [==============================] - 0s 826us/step - loss: 3.1896 - val_loss: 4.1845\n",
            "Epoch 225/500\n",
            "15/15 [==============================] - 0s 828us/step - loss: 2.2931 - val_loss: 4.1207\n",
            "Epoch 226/500\n",
            "15/15 [==============================] - 0s 830us/step - loss: 1.8162 - val_loss: 4.0704\n",
            "Epoch 227/500\n",
            "15/15 [==============================] - 0s 790us/step - loss: 1.4815 - val_loss: 3.8695\n",
            "Epoch 228/500\n",
            "15/15 [==============================] - 0s 768us/step - loss: 1.5744 - val_loss: 4.6900\n",
            "Epoch 229/500\n",
            "15/15 [==============================] - 0s 738us/step - loss: 3.4356 - val_loss: 3.8517\n",
            "Epoch 230/500\n",
            "15/15 [==============================] - 0s 816us/step - loss: 3.0927 - val_loss: 3.9693\n",
            "Epoch 231/500\n",
            "15/15 [==============================] - 0s 815us/step - loss: 1.9476 - val_loss: 3.9439\n",
            "Epoch 232/500\n",
            "15/15 [==============================] - 0s 795us/step - loss: 1.2637 - val_loss: 4.0830\n",
            "Epoch 233/500\n",
            "15/15 [==============================] - 0s 831us/step - loss: 1.1408 - val_loss: 4.2371\n",
            "Epoch 234/500\n",
            "15/15 [==============================] - 0s 822us/step - loss: 1.7138 - val_loss: 4.9474\n",
            "Epoch 235/500\n",
            "15/15 [==============================] - 0s 844us/step - loss: 3.0955 - val_loss: 4.0603\n",
            "Epoch 236/500\n",
            "15/15 [==============================] - 0s 824us/step - loss: 2.7364 - val_loss: 4.2991\n",
            "Epoch 237/500\n",
            "15/15 [==============================] - 0s 858us/step - loss: 2.1198 - val_loss: 4.3222\n",
            "Epoch 238/500\n",
            "15/15 [==============================] - 0s 922us/step - loss: 1.2565 - val_loss: 4.3031\n",
            "Epoch 239/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.7014 - val_loss: 4.5348\n",
            "Epoch 240/500\n",
            "15/15 [==============================] - 0s 874us/step - loss: 3.0757 - val_loss: 5.0867\n",
            "Epoch 241/500\n",
            "15/15 [==============================] - 0s 918us/step - loss: 2.8429 - val_loss: 4.4327\n",
            "Epoch 242/500\n",
            "15/15 [==============================] - 0s 882us/step - loss: 1.6948 - val_loss: 4.0945\n",
            "Epoch 243/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.8781 - val_loss: 4.1341\n",
            "Epoch 244/500\n",
            "15/15 [==============================] - 0s 918us/step - loss: 0.7699 - val_loss: 4.3428\n",
            "Epoch 245/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 1.4402 - val_loss: 4.6352\n",
            "Epoch 246/500\n",
            "15/15 [==============================] - 0s 900us/step - loss: 3.3107 - val_loss: 5.4328\n",
            "Epoch 247/500\n",
            "15/15 [==============================] - 0s 912us/step - loss: 3.1410 - val_loss: 4.1941\n",
            "Epoch 248/500\n",
            "15/15 [==============================] - 0s 946us/step - loss: 1.7008 - val_loss: 4.3566\n",
            "Epoch 249/500\n",
            "15/15 [==============================] - 0s 985us/step - loss: 1.4152 - val_loss: 4.3915\n",
            "Epoch 250/500\n",
            "15/15 [==============================] - 0s 929us/step - loss: 1.9452 - val_loss: 4.2885\n",
            "Epoch 251/500\n",
            "15/15 [==============================] - 0s 919us/step - loss: 2.7703 - val_loss: 5.3062\n",
            "Epoch 252/500\n",
            "15/15 [==============================] - 0s 953us/step - loss: 2.9399 - val_loss: 4.4844\n",
            "Epoch 253/500\n",
            "15/15 [==============================] - 0s 883us/step - loss: 2.0694 - val_loss: 4.3973\n",
            "Epoch 254/500\n",
            "15/15 [==============================] - 0s 878us/step - loss: 2.0326 - val_loss: 4.0263\n",
            "Epoch 255/500\n",
            "15/15 [==============================] - 0s 900us/step - loss: 1.7317 - val_loss: 4.1207\n",
            "Epoch 256/500\n",
            "15/15 [==============================] - 0s 880us/step - loss: 2.3613 - val_loss: 5.1369\n",
            "Epoch 257/500\n",
            "15/15 [==============================] - 0s 956us/step - loss: 3.4699 - val_loss: 3.8787\n",
            "Epoch 258/500\n",
            "15/15 [==============================] - 0s 961us/step - loss: 2.2384 - val_loss: 4.0088\n",
            "Epoch 259/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.9872 - val_loss: 4.5261\n",
            "Epoch 260/500\n",
            "15/15 [==============================] - 0s 893us/step - loss: 1.4911 - val_loss: 5.1905\n",
            "Epoch 261/500\n",
            "15/15 [==============================] - 0s 968us/step - loss: 2.5803 - val_loss: 4.2021\n",
            "Epoch 262/500\n",
            "15/15 [==============================] - 0s 893us/step - loss: 3.6442 - val_loss: 4.5860\n",
            "Epoch 263/500\n",
            "15/15 [==============================] - 0s 876us/step - loss: 1.9328 - val_loss: 3.9538\n",
            "Epoch 264/500\n",
            "15/15 [==============================] - 0s 848us/step - loss: 1.5630 - val_loss: 3.9000\n",
            "Epoch 265/500\n",
            "15/15 [==============================] - 0s 882us/step - loss: 1.7098 - val_loss: 4.8888\n",
            "Epoch 266/500\n",
            "15/15 [==============================] - 0s 930us/step - loss: 3.4477 - val_loss: 3.9836\n",
            "Epoch 267/500\n",
            "15/15 [==============================] - 0s 913us/step - loss: 3.3670 - val_loss: 4.4411\n",
            "Epoch 268/500\n",
            "15/15 [==============================] - 0s 901us/step - loss: 1.7671 - val_loss: 4.0670\n",
            "Epoch 269/500\n",
            "15/15 [==============================] - 0s 885us/step - loss: 0.9901 - val_loss: 4.0998\n",
            "Epoch 270/500\n",
            "15/15 [==============================] - 0s 930us/step - loss: 0.7529 - val_loss: 4.2012\n",
            "Epoch 271/500\n",
            "15/15 [==============================] - 0s 865us/step - loss: 0.8392 - val_loss: 4.2203\n",
            "Epoch 272/500\n",
            "15/15 [==============================] - 0s 913us/step - loss: 1.1151 - val_loss: 5.2791\n",
            "Epoch 273/500\n",
            "15/15 [==============================] - 0s 947us/step - loss: 3.6850 - val_loss: 4.4706\n",
            "Epoch 274/500\n",
            "15/15 [==============================] - 0s 956us/step - loss: 2.8816 - val_loss: 4.3306\n",
            "Epoch 275/500\n",
            "15/15 [==============================] - 0s 952us/step - loss: 1.8999 - val_loss: 3.9690\n",
            "Epoch 276/500\n",
            "15/15 [==============================] - 0s 890us/step - loss: 1.1710 - val_loss: 3.8647\n",
            "Epoch 277/500\n",
            "15/15 [==============================] - 0s 899us/step - loss: 0.9111 - val_loss: 4.0954\n",
            "Epoch 278/500\n",
            "15/15 [==============================] - 0s 910us/step - loss: 1.5575 - val_loss: 4.7605\n",
            "Epoch 279/500\n",
            "15/15 [==============================] - 0s 907us/step - loss: 2.9141 - val_loss: 4.0187\n",
            "Epoch 280/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.0818 - val_loss: 4.4829\n",
            "Epoch 281/500\n",
            "15/15 [==============================] - 0s 954us/step - loss: 2.1551 - val_loss: 4.3780\n",
            "Epoch 282/500\n",
            "15/15 [==============================] - 0s 877us/step - loss: 1.4844 - val_loss: 4.2534\n",
            "Epoch 283/500\n",
            "15/15 [==============================] - 0s 794us/step - loss: 0.9227 - val_loss: 4.4405\n",
            "Epoch 284/500\n",
            "15/15 [==============================] - 0s 905us/step - loss: 1.6653 - val_loss: 4.5939\n",
            "Epoch 285/500\n",
            "15/15 [==============================] - 0s 853us/step - loss: 3.5537 - val_loss: 5.5644\n",
            "Epoch 286/500\n",
            "15/15 [==============================] - 0s 861us/step - loss: 2.9899 - val_loss: 4.0843\n",
            "Epoch 287/500\n",
            "15/15 [==============================] - 0s 861us/step - loss: 1.4772 - val_loss: 3.9914\n",
            "Epoch 288/500\n",
            "15/15 [==============================] - 0s 842us/step - loss: 1.2571 - val_loss: 4.1652\n",
            "Epoch 289/500\n",
            "15/15 [==============================] - 0s 817us/step - loss: 1.4144 - val_loss: 5.1337\n",
            "Epoch 290/500\n",
            "15/15 [==============================] - 0s 826us/step - loss: 3.6963 - val_loss: 4.0933\n",
            "Epoch 291/500\n",
            "15/15 [==============================] - 0s 861us/step - loss: 3.2724 - val_loss: 4.2695\n",
            "Epoch 292/500\n",
            "15/15 [==============================] - 0s 973us/step - loss: 1.9109 - val_loss: 3.9417\n",
            "Epoch 293/500\n",
            "15/15 [==============================] - 0s 795us/step - loss: 0.7063 - val_loss: 4.1725\n",
            "Epoch 294/500\n",
            "15/15 [==============================] - 0s 877us/step - loss: 0.8495 - val_loss: 3.9130\n",
            "Epoch 295/500\n",
            "15/15 [==============================] - 0s 804us/step - loss: 0.9751 - val_loss: 4.0615\n",
            "Epoch 296/500\n",
            "15/15 [==============================] - 0s 875us/step - loss: 1.4264 - val_loss: 5.1291\n",
            "Epoch 297/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.3987 - val_loss: 4.2909\n",
            "Epoch 298/500\n",
            "15/15 [==============================] - 0s 898us/step - loss: 3.4518 - val_loss: 4.2027\n",
            "Epoch 299/500\n",
            "15/15 [==============================] - 0s 925us/step - loss: 2.0981 - val_loss: 3.9682\n",
            "Epoch 300/500\n",
            "15/15 [==============================] - 0s 917us/step - loss: 1.5107 - val_loss: 3.9169\n",
            "Epoch 301/500\n",
            "15/15 [==============================] - 0s 909us/step - loss: 0.8906 - val_loss: 3.8424\n",
            "Epoch 302/500\n",
            "15/15 [==============================] - 0s 981us/step - loss: 1.0830 - val_loss: 4.5717\n",
            "Epoch 303/500\n",
            "15/15 [==============================] - 0s 943us/step - loss: 1.9604 - val_loss: 4.6244\n",
            "Epoch 304/500\n",
            "15/15 [==============================] - 0s 945us/step - loss: 3.4287 - val_loss: 4.6041\n",
            "Epoch 305/500\n",
            "15/15 [==============================] - 0s 899us/step - loss: 2.5797 - val_loss: 4.4060\n",
            "Epoch 306/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 1.8262 - val_loss: 4.2171\n",
            "Epoch 307/500\n",
            "15/15 [==============================] - 0s 906us/step - loss: 1.2125 - val_loss: 4.5280\n",
            "Epoch 308/500\n",
            "15/15 [==============================] - 0s 828us/step - loss: 1.3743 - val_loss: 4.9872\n",
            "Epoch 309/500\n",
            "15/15 [==============================] - 0s 849us/step - loss: 3.3682 - val_loss: 5.2878\n",
            "Epoch 310/500\n",
            "15/15 [==============================] - 0s 848us/step - loss: 3.2406 - val_loss: 4.2029\n",
            "Epoch 311/500\n",
            "15/15 [==============================] - 0s 870us/step - loss: 1.8800 - val_loss: 4.2748\n",
            "Epoch 312/500\n",
            "15/15 [==============================] - 0s 838us/step - loss: 1.1306 - val_loss: 3.8950\n",
            "Epoch 313/500\n",
            "15/15 [==============================] - 0s 832us/step - loss: 0.9901 - val_loss: 4.5300\n",
            "Epoch 314/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 2.5539 - val_loss: 4.0096\n",
            "Epoch 315/500\n",
            "15/15 [==============================] - 0s 832us/step - loss: 3.3588 - val_loss: 4.8898\n",
            "Epoch 316/500\n",
            "15/15 [==============================] - 0s 941us/step - loss: 2.3954 - val_loss: 4.1444\n",
            "Epoch 317/500\n",
            "15/15 [==============================] - 0s 805us/step - loss: 1.1939 - val_loss: 4.0518\n",
            "Epoch 318/500\n",
            "15/15 [==============================] - 0s 818us/step - loss: 0.6631 - val_loss: 4.1691\n",
            "Epoch 319/500\n",
            "15/15 [==============================] - 0s 855us/step - loss: 1.1985 - val_loss: 4.2500\n",
            "Epoch 320/500\n",
            "15/15 [==============================] - 0s 765us/step - loss: 1.3913 - val_loss: 4.5714\n",
            "Epoch 321/500\n",
            "15/15 [==============================] - 0s 784us/step - loss: 2.9696 - val_loss: 5.0735\n",
            "Epoch 322/500\n",
            "15/15 [==============================] - 0s 789us/step - loss: 3.2684 - val_loss: 4.2454\n",
            "Epoch 323/500\n",
            "15/15 [==============================] - 0s 800us/step - loss: 1.9492 - val_loss: 4.2294\n",
            "Epoch 324/500\n",
            "15/15 [==============================] - 0s 817us/step - loss: 1.2118 - val_loss: 4.0896\n",
            "Epoch 325/500\n",
            "15/15 [==============================] - 0s 763us/step - loss: 0.7552 - val_loss: 4.4335\n",
            "Epoch 326/500\n",
            "15/15 [==============================] - 0s 811us/step - loss: 1.6264 - val_loss: 4.3124\n",
            "Epoch 327/500\n",
            "15/15 [==============================] - 0s 760us/step - loss: 2.8042 - val_loss: 5.5901\n",
            "Epoch 328/500\n",
            "15/15 [==============================] - 0s 848us/step - loss: 3.4576 - val_loss: 4.1816\n",
            "Epoch 329/500\n",
            "15/15 [==============================] - 0s 804us/step - loss: 2.2351 - val_loss: 3.9537\n",
            "Epoch 330/500\n",
            "15/15 [==============================] - 0s 835us/step - loss: 1.2390 - val_loss: 4.0800\n",
            "Epoch 331/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.6679 - val_loss: 4.0956\n",
            "Epoch 332/500\n",
            "15/15 [==============================] - 0s 856us/step - loss: 0.6297 - val_loss: 4.0393\n",
            "Epoch 333/500\n",
            "15/15 [==============================] - 0s 805us/step - loss: 1.0248 - val_loss: 3.9861\n",
            "Epoch 334/500\n",
            "15/15 [==============================] - 0s 774us/step - loss: 1.0723 - val_loss: 4.1479\n",
            "Epoch 335/500\n",
            "15/15 [==============================] - 0s 858us/step - loss: 3.1434 - val_loss: 4.5010\n",
            "Epoch 336/500\n",
            "15/15 [==============================] - 0s 823us/step - loss: 3.6672 - val_loss: 4.0382\n",
            "Epoch 337/500\n",
            "15/15 [==============================] - 0s 823us/step - loss: 1.6062 - val_loss: 4.1818\n",
            "Epoch 338/500\n",
            "15/15 [==============================] - 0s 812us/step - loss: 1.2321 - val_loss: 4.0479\n",
            "Epoch 339/500\n",
            "15/15 [==============================] - 0s 813us/step - loss: 0.7374 - val_loss: 4.0699\n",
            "Epoch 340/500\n",
            "15/15 [==============================] - 0s 853us/step - loss: 0.6715 - val_loss: 4.0496\n",
            "Epoch 341/500\n",
            "15/15 [==============================] - 0s 862us/step - loss: 0.9819 - val_loss: 4.0556\n",
            "Epoch 342/500\n",
            "15/15 [==============================] - 0s 861us/step - loss: 1.6799 - val_loss: 5.0229\n",
            "Epoch 343/500\n",
            "15/15 [==============================] - 0s 897us/step - loss: 3.2818 - val_loss: 4.7143\n",
            "Epoch 344/500\n",
            "15/15 [==============================] - 0s 976us/step - loss: 3.7697 - val_loss: 4.0946\n",
            "Epoch 345/500\n",
            "15/15 [==============================] - 0s 909us/step - loss: 1.6207 - val_loss: 4.1130\n",
            "Epoch 346/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.1539 - val_loss: 4.2113\n",
            "Epoch 347/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.6509 - val_loss: 4.2516\n",
            "Epoch 348/500\n",
            "15/15 [==============================] - 0s 925us/step - loss: 1.6963 - val_loss: 3.9630\n",
            "Epoch 349/500\n",
            "15/15 [==============================] - 0s 965us/step - loss: 2.8805 - val_loss: 4.8262\n",
            "Epoch 350/500\n",
            "15/15 [==============================] - 0s 997us/step - loss: 3.2725 - val_loss: 4.0846\n",
            "Epoch 351/500\n",
            "15/15 [==============================] - 0s 915us/step - loss: 2.2569 - val_loss: 4.1119\n",
            "Epoch 352/500\n",
            "15/15 [==============================] - 0s 854us/step - loss: 1.0998 - val_loss: 4.0956\n",
            "Epoch 353/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.7802 - val_loss: 4.0339\n",
            "Epoch 354/500\n",
            "15/15 [==============================] - 0s 752us/step - loss: 0.7328 - val_loss: 4.1468\n",
            "Epoch 355/500\n",
            "15/15 [==============================] - 0s 845us/step - loss: 1.0728 - val_loss: 4.1547\n",
            "Epoch 356/500\n",
            "15/15 [==============================] - 0s 835us/step - loss: 0.9490 - val_loss: 4.8889\n",
            "Epoch 357/500\n",
            "15/15 [==============================] - 0s 788us/step - loss: 2.7406 - val_loss: 4.0320\n",
            "Epoch 358/500\n",
            "15/15 [==============================] - 0s 801us/step - loss: 3.1517 - val_loss: 4.6787\n",
            "Epoch 359/500\n",
            "15/15 [==============================] - 0s 829us/step - loss: 2.4251 - val_loss: 4.3100\n",
            "Epoch 360/500\n",
            "15/15 [==============================] - 0s 815us/step - loss: 1.4556 - val_loss: 4.3139\n",
            "Epoch 361/500\n",
            "15/15 [==============================] - 0s 869us/step - loss: 1.2478 - val_loss: 4.5914\n",
            "Epoch 362/500\n",
            "15/15 [==============================] - 0s 821us/step - loss: 2.3530 - val_loss: 3.9937\n",
            "Epoch 363/500\n",
            "15/15 [==============================] - 0s 867us/step - loss: 2.8945 - val_loss: 4.9368\n",
            "Epoch 364/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 2.5621 - val_loss: 4.7623\n",
            "Epoch 365/500\n",
            "15/15 [==============================] - 0s 875us/step - loss: 2.4414 - val_loss: 4.2703\n",
            "Epoch 366/500\n",
            "15/15 [==============================] - 0s 831us/step - loss: 1.9328 - val_loss: 4.0291\n",
            "Epoch 367/500\n",
            "15/15 [==============================] - 0s 782us/step - loss: 1.8663 - val_loss: 4.0538\n",
            "Epoch 368/500\n",
            "15/15 [==============================] - 0s 802us/step - loss: 1.8862 - val_loss: 4.8945\n",
            "Epoch 369/500\n",
            "15/15 [==============================] - 0s 853us/step - loss: 3.2460 - val_loss: 3.9537\n",
            "Epoch 370/500\n",
            "15/15 [==============================] - 0s 749us/step - loss: 2.8157 - val_loss: 4.2241\n",
            "Epoch 371/500\n",
            "15/15 [==============================] - 0s 831us/step - loss: 1.9085 - val_loss: 4.1946\n",
            "Epoch 372/500\n",
            "15/15 [==============================] - 0s 813us/step - loss: 1.2860 - val_loss: 4.2448\n",
            "Epoch 373/500\n",
            "15/15 [==============================] - 0s 828us/step - loss: 1.1956 - val_loss: 5.2682\n",
            "Epoch 374/500\n",
            "15/15 [==============================] - 0s 767us/step - loss: 2.9290 - val_loss: 4.1630\n",
            "Epoch 375/500\n",
            "15/15 [==============================] - 0s 774us/step - loss: 3.7273 - val_loss: 4.5375\n",
            "Epoch 376/500\n",
            "15/15 [==============================] - 0s 793us/step - loss: 2.2056 - val_loss: 4.0956\n",
            "Epoch 377/500\n",
            "15/15 [==============================] - 0s 811us/step - loss: 1.5453 - val_loss: 3.9268\n",
            "Epoch 378/500\n",
            "15/15 [==============================] - 0s 771us/step - loss: 1.1253 - val_loss: 4.1617\n",
            "Epoch 379/500\n",
            "15/15 [==============================] - 0s 852us/step - loss: 2.3356 - val_loss: 4.4640\n",
            "Epoch 380/500\n",
            "15/15 [==============================] - 0s 792us/step - loss: 3.6082 - val_loss: 4.3825\n",
            "Epoch 381/500\n",
            "15/15 [==============================] - 0s 814us/step - loss: 2.3788 - val_loss: 4.5258\n",
            "Epoch 382/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.6029 - val_loss: 4.0360\n",
            "Epoch 383/500\n",
            "15/15 [==============================] - 0s 885us/step - loss: 0.7313 - val_loss: 4.0277\n",
            "Epoch 384/500\n",
            "15/15 [==============================] - 0s 962us/step - loss: 0.5437 - val_loss: 4.5095\n",
            "Epoch 385/500\n",
            "15/15 [==============================] - 0s 961us/step - loss: 1.3722 - val_loss: 5.0178\n",
            "Epoch 386/500\n",
            "15/15 [==============================] - 0s 959us/step - loss: 3.2816 - val_loss: 4.8477\n",
            "Epoch 387/500\n",
            "15/15 [==============================] - 0s 971us/step - loss: 3.2176 - val_loss: 4.4889\n",
            "Epoch 388/500\n",
            "15/15 [==============================] - 0s 919us/step - loss: 1.8668 - val_loss: 4.2867\n",
            "Epoch 389/500\n",
            "15/15 [==============================] - 0s 892us/step - loss: 1.2495 - val_loss: 4.0880\n",
            "Epoch 390/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.8427 - val_loss: 4.3263\n",
            "Epoch 391/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 2.1790 - val_loss: 3.9993\n",
            "Epoch 392/500\n",
            "15/15 [==============================] - 0s 895us/step - loss: 2.7395 - val_loss: 5.1805\n",
            "Epoch 393/500\n",
            "15/15 [==============================] - 0s 854us/step - loss: 2.5641 - val_loss: 4.5878\n",
            "Epoch 394/500\n",
            "15/15 [==============================] - 0s 909us/step - loss: 2.2450 - val_loss: 4.1993\n",
            "Epoch 395/500\n",
            "15/15 [==============================] - 0s 916us/step - loss: 2.0030 - val_loss: 3.8969\n",
            "Epoch 396/500\n",
            "15/15 [==============================] - 0s 910us/step - loss: 1.6641 - val_loss: 3.8968\n",
            "Epoch 397/500\n",
            "15/15 [==============================] - 0s 931us/step - loss: 1.8169 - val_loss: 4.7163\n",
            "Epoch 398/500\n",
            "15/15 [==============================] - 0s 969us/step - loss: 3.2801 - val_loss: 4.0019\n",
            "Epoch 399/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 2.9925 - val_loss: 4.0467\n",
            "Epoch 400/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 1.7129 - val_loss: 3.9499\n",
            "Epoch 401/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.8114 - val_loss: 4.0404\n",
            "Epoch 402/500\n",
            "15/15 [==============================] - 0s 999us/step - loss: 0.9763 - val_loss: 4.1192\n",
            "Epoch 403/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 1.8921 - val_loss: 5.2811\n",
            "Epoch 404/500\n",
            "15/15 [==============================] - 0s 992us/step - loss: 3.4415 - val_loss: 4.0447\n",
            "Epoch 405/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.7506 - val_loss: 4.2864\n",
            "Epoch 406/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 1.9753 - val_loss: 4.2347\n",
            "Epoch 407/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 1.3567 - val_loss: 4.1678\n",
            "Epoch 408/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 1.3801 - val_loss: 4.7630\n",
            "Epoch 409/500\n",
            "15/15 [==============================] - 0s 952us/step - loss: 3.3505 - val_loss: 5.4104\n",
            "Epoch 410/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.9355 - val_loss: 4.3629\n",
            "Epoch 411/500\n",
            "15/15 [==============================] - 0s 894us/step - loss: 1.8274 - val_loss: 4.1026\n",
            "Epoch 412/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.8744 - val_loss: 4.1160\n",
            "Epoch 413/500\n",
            "15/15 [==============================] - 0s 895us/step - loss: 1.1525 - val_loss: 4.4814\n",
            "Epoch 414/500\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1.3754 - val_loss: 4.6401\n",
            "Epoch 415/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.9534 - val_loss: 4.7479\n",
            "Epoch 416/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.6492 - val_loss: 4.6215\n",
            "Epoch 417/500\n",
            "15/15 [==============================] - 0s 971us/step - loss: 2.2493 - val_loss: 4.4147\n",
            "Epoch 418/500\n",
            "15/15 [==============================] - 0s 889us/step - loss: 1.9194 - val_loss: 4.2699\n",
            "Epoch 419/500\n",
            "15/15 [==============================] - 0s 931us/step - loss: 1.7601 - val_loss: 4.2076\n",
            "Epoch 420/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.3658 - val_loss: 5.3849\n",
            "Epoch 421/500\n",
            "15/15 [==============================] - 0s 968us/step - loss: 3.4476 - val_loss: 4.1418\n",
            "Epoch 422/500\n",
            "15/15 [==============================] - 0s 882us/step - loss: 2.2339 - val_loss: 4.0689\n",
            "Epoch 423/500\n",
            "15/15 [==============================] - 0s 847us/step - loss: 0.9941 - val_loss: 4.0695\n",
            "Epoch 424/500\n",
            "15/15 [==============================] - 0s 879us/step - loss: 0.9267 - val_loss: 3.9820\n",
            "Epoch 425/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.1058 - val_loss: 3.8172\n",
            "Epoch 426/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.8680 - val_loss: 4.0833\n",
            "Epoch 427/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 1.8369 - val_loss: 4.9782\n",
            "Epoch 428/500\n",
            "15/15 [==============================] - 0s 929us/step - loss: 4.0349 - val_loss: 4.7459\n",
            "Epoch 429/500\n",
            "15/15 [==============================] - 0s 912us/step - loss: 2.8221 - val_loss: 3.9670\n",
            "Epoch 430/500\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1.5435 - val_loss: 4.0434\n",
            "Epoch 431/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.8147 - val_loss: 3.9980\n",
            "Epoch 432/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.8428 - val_loss: 4.0943\n",
            "Epoch 433/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.9620 - val_loss: 4.1560\n",
            "Epoch 434/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 1.7552 - val_loss: 4.9129\n",
            "Epoch 435/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5291 - val_loss: 4.1449\n",
            "Epoch 436/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.0864 - val_loss: 4.2793\n",
            "Epoch 437/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 1.8149 - val_loss: 4.0730\n",
            "Epoch 438/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.9338 - val_loss: 4.0419\n",
            "Epoch 439/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.6018 - val_loss: 3.9165\n",
            "Epoch 440/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.9319 - val_loss: 3.9674\n",
            "Epoch 441/500\n",
            "15/15 [==============================] - 0s 994us/step - loss: 0.8594 - val_loss: 4.1505\n",
            "Epoch 442/500\n",
            "15/15 [==============================] - 0s 998us/step - loss: 2.3121 - val_loss: 5.0759\n",
            "Epoch 443/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2165 - val_loss: 4.3769\n",
            "Epoch 444/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.2612 - val_loss: 3.9829\n",
            "Epoch 445/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 1.0218 - val_loss: 4.1708\n",
            "Epoch 446/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 0.9000 - val_loss: 4.0020\n",
            "Epoch 447/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 1.2459 - val_loss: 3.9709\n",
            "Epoch 448/500\n",
            "15/15 [==============================] - 0s 947us/step - loss: 1.4425 - val_loss: 4.1462\n",
            "Epoch 449/500\n",
            "15/15 [==============================] - 0s 806us/step - loss: 1.5037 - val_loss: 5.0146\n",
            "Epoch 450/500\n",
            "15/15 [==============================] - 0s 930us/step - loss: 2.9750 - val_loss: 4.3302\n",
            "Epoch 451/500\n",
            "15/15 [==============================] - 0s 884us/step - loss: 3.3871 - val_loss: 4.1344\n",
            "Epoch 452/500\n",
            "15/15 [==============================] - 0s 880us/step - loss: 2.4357 - val_loss: 4.2512\n",
            "Epoch 453/500\n",
            "15/15 [==============================] - 0s 860us/step - loss: 1.5564 - val_loss: 4.0491\n",
            "Epoch 454/500\n",
            "15/15 [==============================] - 0s 855us/step - loss: 0.7921 - val_loss: 4.0438\n",
            "Epoch 455/500\n",
            "15/15 [==============================] - 0s 854us/step - loss: 1.1984 - val_loss: 4.5252\n",
            "Epoch 456/500\n",
            "15/15 [==============================] - 0s 866us/step - loss: 2.6082 - val_loss: 4.0093\n",
            "Epoch 457/500\n",
            "15/15 [==============================] - 0s 855us/step - loss: 3.3595 - val_loss: 5.5814\n",
            "Epoch 458/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 2.4327 - val_loss: 4.1944\n",
            "Epoch 459/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 1.3602 - val_loss: 4.0262\n",
            "Epoch 460/500\n",
            "15/15 [==============================] - 0s 853us/step - loss: 0.7066 - val_loss: 4.2707\n",
            "Epoch 461/500\n",
            "15/15 [==============================] - 0s 892us/step - loss: 1.2226 - val_loss: 4.4118\n",
            "Epoch 462/500\n",
            "15/15 [==============================] - 0s 902us/step - loss: 1.8719 - val_loss: 4.5660\n",
            "Epoch 463/500\n",
            "15/15 [==============================] - 0s 820us/step - loss: 3.7704 - val_loss: 5.0221\n",
            "Epoch 464/500\n",
            "15/15 [==============================] - 0s 883us/step - loss: 2.7567 - val_loss: 4.5084\n",
            "Epoch 465/500\n",
            "15/15 [==============================] - 0s 816us/step - loss: 1.9253 - val_loss: 4.4947\n",
            "Epoch 466/500\n",
            "15/15 [==============================] - 0s 837us/step - loss: 1.1299 - val_loss: 4.2786\n",
            "Epoch 467/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 1.0927 - val_loss: 4.3818\n",
            "Epoch 468/500\n",
            "15/15 [==============================] - 0s 885us/step - loss: 1.4744 - val_loss: 4.0638\n",
            "Epoch 469/500\n",
            "15/15 [==============================] - 0s 812us/step - loss: 2.6185 - val_loss: 5.7069\n",
            "Epoch 470/500\n",
            "15/15 [==============================] - 0s 841us/step - loss: 3.9328 - val_loss: 3.9996\n",
            "Epoch 471/500\n",
            "15/15 [==============================] - 0s 809us/step - loss: 2.0834 - val_loss: 4.0517\n",
            "Epoch 472/500\n",
            "15/15 [==============================] - 0s 811us/step - loss: 1.2218 - val_loss: 4.2425\n",
            "Epoch 473/500\n",
            "15/15 [==============================] - 0s 763us/step - loss: 1.0489 - val_loss: 4.2161\n",
            "Epoch 474/500\n",
            "15/15 [==============================] - 0s 855us/step - loss: 0.7286 - val_loss: 4.6303\n",
            "Epoch 475/500\n",
            "15/15 [==============================] - 0s 842us/step - loss: 2.3336 - val_loss: 4.0216\n",
            "Epoch 476/500\n",
            "15/15 [==============================] - 0s 886us/step - loss: 3.0629 - val_loss: 4.6286\n",
            "Epoch 477/500\n",
            "15/15 [==============================] - 0s 896us/step - loss: 2.4182 - val_loss: 4.3519\n",
            "Epoch 478/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 1.8335 - val_loss: 4.5660\n",
            "Epoch 479/500\n",
            "15/15 [==============================] - 0s 994us/step - loss: 2.3685 - val_loss: 4.2040\n",
            "Epoch 480/500\n",
            "15/15 [==============================] - 0s 893us/step - loss: 2.1945 - val_loss: 4.1073\n",
            "Epoch 481/500\n",
            "15/15 [==============================] - 0s 973us/step - loss: 2.1347 - val_loss: 5.7341\n",
            "Epoch 482/500\n",
            "15/15 [==============================] - 0s 962us/step - loss: 2.7570 - val_loss: 4.5784\n",
            "Epoch 483/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.3659 - val_loss: 4.6316\n",
            "Epoch 484/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 2.0476 - val_loss: 4.2441\n",
            "Epoch 485/500\n",
            "15/15 [==============================] - 0s 937us/step - loss: 1.0683 - val_loss: 3.9342\n",
            "Epoch 486/500\n",
            "15/15 [==============================] - 0s 980us/step - loss: 1.1375 - val_loss: 4.7919\n",
            "Epoch 487/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.8776 - val_loss: 4.0536\n",
            "Epoch 488/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.4592 - val_loss: 4.2774\n",
            "Epoch 489/500\n",
            "15/15 [==============================] - 0s 899us/step - loss: 1.8599 - val_loss: 4.8747\n",
            "Epoch 490/500\n",
            "15/15 [==============================] - 0s 850us/step - loss: 2.3778 - val_loss: 4.2643\n",
            "Epoch 491/500\n",
            "15/15 [==============================] - 0s 820us/step - loss: 2.3801 - val_loss: 3.8863\n",
            "Epoch 492/500\n",
            "15/15 [==============================] - 0s 804us/step - loss: 2.4971 - val_loss: 4.6511\n",
            "Epoch 493/500\n",
            "15/15 [==============================] - 0s 925us/step - loss: 2.1463 - val_loss: 4.3228\n",
            "Epoch 494/500\n",
            "15/15 [==============================] - 0s 827us/step - loss: 2.3163 - val_loss: 3.9952\n",
            "Epoch 495/500\n",
            "15/15 [==============================] - 0s 829us/step - loss: 2.3208 - val_loss: 4.6212\n",
            "Epoch 496/500\n",
            "15/15 [==============================] - 0s 857us/step - loss: 2.5777 - val_loss: 4.6754\n",
            "Epoch 497/500\n",
            "15/15 [==============================] - 0s 815us/step - loss: 2.5770 - val_loss: 4.1628\n",
            "Epoch 498/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 2.4626 - val_loss: 4.3892\n",
            "Epoch 499/500\n",
            "15/15 [==============================] - 0s 773us/step - loss: 2.0597 - val_loss: 4.4272\n",
            "Epoch 500/500\n",
            "15/15 [==============================] - 0s 772us/step - loss: 1.3679 - val_loss: 4.7002\n",
            "5/5 [==============================] - 0s 758us/step\n",
            "2/2 [==============================] - 0s 846us/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x2999b5ed0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas\n",
        "import tensorflow as tf\n",
        "\n",
        "### BEG DATA INTAKE\n",
        "# read data file and confirm data shape\n",
        "dataframe = pandas.read_csv('https://raw.githubusercontent.com/bryankolaczkowski/ALS3200C/main/phenopred.data.csv')\n",
        "print(dataframe.shape)\n",
        "\n",
        "# split into training and validation subsets, and confirm shape\n",
        "train_dataframe = dataframe.sample(frac=0.8, random_state=402201)\n",
        "valid_dataframe = dataframe.drop(train_dataframe.index)\n",
        "print(train_dataframe.shape, valid_dataframe.shape, dataframe.shape)\n",
        "\n",
        "# extract explanatory variables, convert to numpy and confirm shapes\n",
        "snp_ids = [ x for x in dataframe.columns if x.find('SNP') == 0 ]\n",
        "train_x = train_dataframe[snp_ids].to_numpy()\n",
        "valid_x = valid_dataframe[snp_ids].to_numpy()\n",
        "print(train_x.shape, valid_x.shape)\n",
        "\n",
        "# extract response variables, convert to numpy and confirm shapes\n",
        "train_y = train_dataframe['LS'].to_numpy()\n",
        "valid_y = valid_dataframe['LS'].to_numpy()\n",
        "print(train_y.shape, valid_y.shape)\n",
        "\n",
        "# package into tensorflow.Dataset objects and batch\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_x,train_y)).batch(10)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x,valid_y)).batch(36)\n",
        "### END DATA INTAKE\n",
        "\n",
        "### BEG NEURAL NETWORK TRAIN-VALIDATE\n",
        "## build and compile linear neural-network model\n",
        "model = tf.keras.models.Sequential()\n",
        "### BEG BUILD AND COMPILE MODEL\n",
        "model.add(tf.keras.layers.Dense(1, input_shape=(17165,)))\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=tf.keras.losses.MeanAbsoluteError())\n",
        "### END BUILD AND COMPILE MODEL\n",
        "model.summary()\n",
        "\n",
        "## fit and validate neural-network model\n",
        "model.fit(train_data, epochs=500, validation_data=valid_data)\n",
        "### END NEURAL NETWORK TRAIN-VALIDATE\n",
        "\n",
        "### PLOT RESULTS FOR VISUALIZATION\n",
        "## predict training and validation responses for plotting\n",
        "train_y_hat = model.predict(train_x)\n",
        "valid_y_hat = model.predict(valid_x)\n",
        "\n",
        "## plot true-vs-predicted responses\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot([10,60],[10,60])\n",
        "plt.scatter(train_y, train_y_hat, marker='o')\n",
        "plt.scatter(valid_y, valid_y_hat, marker='+')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2Dgi6Rpvduw"
      },
      "source": [
        "## Mitigating Overfitting with Input Layer Dropout\n",
        "\n",
        "A common challenge when training on limited genetic data is overfitting, where the model memorizes noise and biases in the training set. This reduces generalizability to new data.\n",
        "\n",
        "To mitigate overfitting, we apply dropout - randomized exclusion of inputs - on the SNP genotype data during training. This relies on the redundancy of information across the large number of variants.\n",
        "\n",
        "Specifically, an InputLayer is first added to explicitly define the input shape. A Dropout layer is then applied, set to aggressively drop 90% of SNP values on each batch.\n",
        "\n",
        "This approximately reduces the informative dimensionality from 17,165 down to 1,726. However, critically, all SNPs are retained during validation and testing.\n",
        "\n",
        "By stochastically removing inputs, the network is prevented from relying on particular variants as predictive signals. This encourages more robust fitting based on additive effects across many SNPs.\n",
        "\n",
        "While 90% dropout is extreme, the vast discrepancy between sample size (N=146) and SNPs (P=17,165) necessitates aggressive regularization. More modest dropout rates of 0.2-0.5 are commonly used with larger datasets.\n",
        "\n",
        "Preliminary results indicate this input dropout effectively reduces overfitting, as measured by the similarity between training and validation losses after fitting. Future work should further confirm these improvements on independent test data.\n",
        "\n",
        "Overall, input layer dropout provides a straightforward mechanism to improve genetic model generalization in the context of limited sample sizes, a common challenge in genomics applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K16dqkzjv0MH",
        "outputId": "759af986-3737-4e2e-cf30-748c932d2853"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(182, 17167)\n",
            "(146, 17167) (36, 17167) (182, 17167)\n",
            "(146, 17165) (36, 17165)\n",
            "(146,) (36,)\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dropout (Dropout)           (None, 17165)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 17166     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17166 (67.05 KB)\n",
            "Trainable params: 17166 (67.05 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 24.8134 - val_loss: 14.1081\n",
            "Epoch 2/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 8.7763 - val_loss: 7.6724\n",
            "Epoch 3/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 7.1728 - val_loss: 6.7986\n",
            "Epoch 4/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 5.7317 - val_loss: 6.1993\n",
            "Epoch 5/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.6534 - val_loss: 4.8621\n",
            "Epoch 6/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.8870 - val_loss: 4.4449\n",
            "Epoch 7/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.8103 - val_loss: 3.9521\n",
            "Epoch 8/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6218 - val_loss: 3.8815\n",
            "Epoch 9/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.4785 - val_loss: 3.8237\n",
            "Epoch 10/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8918 - val_loss: 4.0319\n",
            "Epoch 11/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7669 - val_loss: 3.8528\n",
            "Epoch 12/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.4182 - val_loss: 3.9198\n",
            "Epoch 13/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9529 - val_loss: 3.8825\n",
            "Epoch 14/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5410 - val_loss: 3.6881\n",
            "Epoch 15/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.3935 - val_loss: 3.7116\n",
            "Epoch 16/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6164 - val_loss: 3.7394\n",
            "Epoch 17/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.0066 - val_loss: 3.7445\n",
            "Epoch 18/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.3740 - val_loss: 3.7396\n",
            "Epoch 19/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5311 - val_loss: 3.7101\n",
            "Epoch 20/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.3052 - val_loss: 3.6723\n",
            "Epoch 21/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 2.9764 - val_loss: 3.7314\n",
            "Epoch 22/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.2565 - val_loss: 3.8924\n",
            "Epoch 23/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.1748 - val_loss: 3.7880\n",
            "Epoch 24/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.1229 - val_loss: 3.8195\n",
            "Epoch 25/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6098 - val_loss: 3.9291\n",
            "Epoch 26/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.3027 - val_loss: 3.9511\n",
            "Epoch 27/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6728 - val_loss: 3.8020\n",
            "Epoch 28/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.3533 - val_loss: 3.7607\n",
            "Epoch 29/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5764 - val_loss: 3.7963\n",
            "Epoch 30/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8504 - val_loss: 3.7569\n",
            "Epoch 31/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5274 - val_loss: 3.8853\n",
            "Epoch 32/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.8588 - val_loss: 3.9079\n",
            "Epoch 33/500\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 3.4326 - val_loss: 3.8139\n",
            "Epoch 34/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.5333 - val_loss: 4.0189\n",
            "Epoch 35/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5326 - val_loss: 3.8816\n",
            "Epoch 36/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.2862 - val_loss: 3.9570\n",
            "Epoch 37/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.3552 - val_loss: 3.8915\n",
            "Epoch 38/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.4857 - val_loss: 3.9559\n",
            "Epoch 39/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7787 - val_loss: 3.8345\n",
            "Epoch 40/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.4282 - val_loss: 3.7609\n",
            "Epoch 41/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.2500 - val_loss: 3.7722\n",
            "Epoch 42/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5980 - val_loss: 4.2663\n",
            "Epoch 43/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5914 - val_loss: 3.7027\n",
            "Epoch 44/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7266 - val_loss: 3.6404\n",
            "Epoch 45/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.4593 - val_loss: 3.6456\n",
            "Epoch 46/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6641 - val_loss: 3.6459\n",
            "Epoch 47/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.4461 - val_loss: 3.6863\n",
            "Epoch 48/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.4578 - val_loss: 3.7125\n",
            "Epoch 49/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.4432 - val_loss: 3.6595\n",
            "Epoch 50/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5000 - val_loss: 3.7402\n",
            "Epoch 51/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7014 - val_loss: 3.7593\n",
            "Epoch 52/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.4934 - val_loss: 4.0106\n",
            "Epoch 53/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7305 - val_loss: 3.6949\n",
            "Epoch 54/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7224 - val_loss: 3.8111\n",
            "Epoch 55/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.2343 - val_loss: 3.7373\n",
            "Epoch 56/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7593 - val_loss: 3.7469\n",
            "Epoch 57/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.4935 - val_loss: 3.7174\n",
            "Epoch 58/500\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 3.7277 - val_loss: 3.7649\n",
            "Epoch 59/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6410 - val_loss: 3.6942\n",
            "Epoch 60/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8410 - val_loss: 3.7034\n",
            "Epoch 61/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9437 - val_loss: 3.7455\n",
            "Epoch 62/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0451 - val_loss: 3.7985\n",
            "Epoch 63/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.6563 - val_loss: 3.7268\n",
            "Epoch 64/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5217 - val_loss: 3.7960\n",
            "Epoch 65/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8601 - val_loss: 3.8855\n",
            "Epoch 66/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.4857 - val_loss: 4.0108\n",
            "Epoch 67/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8297 - val_loss: 3.8962\n",
            "Epoch 68/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6489 - val_loss: 3.8492\n",
            "Epoch 69/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9413 - val_loss: 3.9988\n",
            "Epoch 70/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.7257 - val_loss: 3.7743\n",
            "Epoch 71/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5375 - val_loss: 3.8577\n",
            "Epoch 72/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6639 - val_loss: 3.9341\n",
            "Epoch 73/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5952 - val_loss: 3.8038\n",
            "Epoch 74/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6909 - val_loss: 3.8712\n",
            "Epoch 75/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7855 - val_loss: 3.7491\n",
            "Epoch 76/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2646 - val_loss: 3.7417\n",
            "Epoch 77/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7738 - val_loss: 3.8855\n",
            "Epoch 78/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1308 - val_loss: 3.7570\n",
            "Epoch 79/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9151 - val_loss: 3.6994\n",
            "Epoch 80/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2778 - val_loss: 3.6940\n",
            "Epoch 81/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7318 - val_loss: 3.7344\n",
            "Epoch 82/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.4515 - val_loss: 3.7877\n",
            "Epoch 83/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.4954 - val_loss: 3.9305\n",
            "Epoch 84/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6629 - val_loss: 3.8305\n",
            "Epoch 85/500\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 3.9758 - val_loss: 3.8906\n",
            "Epoch 86/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7738 - val_loss: 3.8662\n",
            "Epoch 87/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5106 - val_loss: 3.8002\n",
            "Epoch 88/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8129 - val_loss: 3.8450\n",
            "Epoch 89/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9136 - val_loss: 3.9801\n",
            "Epoch 90/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9102 - val_loss: 3.8834\n",
            "Epoch 91/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9262 - val_loss: 4.3701\n",
            "Epoch 92/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.4131 - val_loss: 4.2919\n",
            "Epoch 93/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.9539 - val_loss: 4.0850\n",
            "Epoch 94/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8770 - val_loss: 3.9590\n",
            "Epoch 95/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7328 - val_loss: 4.0393\n",
            "Epoch 96/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9010 - val_loss: 4.1866\n",
            "Epoch 97/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0510 - val_loss: 3.9037\n",
            "Epoch 98/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.5255 - val_loss: 3.8667\n",
            "Epoch 99/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1341 - val_loss: 3.8408\n",
            "Epoch 100/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8444 - val_loss: 3.8158\n",
            "Epoch 101/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6641 - val_loss: 3.8207\n",
            "Epoch 102/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1554 - val_loss: 3.9923\n",
            "Epoch 103/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2468 - val_loss: 3.6650\n",
            "Epoch 104/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8067 - val_loss: 3.8173\n",
            "Epoch 105/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7499 - val_loss: 3.6479\n",
            "Epoch 106/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6128 - val_loss: 3.7847\n",
            "Epoch 107/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6033 - val_loss: 3.6469\n",
            "Epoch 108/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6049 - val_loss: 3.6242\n",
            "Epoch 109/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1995 - val_loss: 3.7189\n",
            "Epoch 110/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2278 - val_loss: 3.7693\n",
            "Epoch 111/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.8518 - val_loss: 3.7980\n",
            "Epoch 112/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7982 - val_loss: 3.8158\n",
            "Epoch 113/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1925 - val_loss: 3.7772\n",
            "Epoch 114/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1468 - val_loss: 3.7239\n",
            "Epoch 115/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5609 - val_loss: 4.2138\n",
            "Epoch 116/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0947 - val_loss: 4.1978\n",
            "Epoch 117/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0318 - val_loss: 3.8320\n",
            "Epoch 118/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9365 - val_loss: 3.7877\n",
            "Epoch 119/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2289 - val_loss: 3.7880\n",
            "Epoch 120/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7365 - val_loss: 3.8187\n",
            "Epoch 121/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6970 - val_loss: 3.8011\n",
            "Epoch 122/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6845 - val_loss: 4.2360\n",
            "Epoch 123/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9759 - val_loss: 3.9015\n",
            "Epoch 124/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8560 - val_loss: 4.2221\n",
            "Epoch 125/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7807 - val_loss: 3.8089\n",
            "Epoch 126/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5169 - val_loss: 3.8138\n",
            "Epoch 127/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6326 - val_loss: 3.8830\n",
            "Epoch 128/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5624 - val_loss: 3.8241\n",
            "Epoch 129/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3941 - val_loss: 3.7875\n",
            "Epoch 130/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9418 - val_loss: 3.7747\n",
            "Epoch 131/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3069 - val_loss: 3.8448\n",
            "Epoch 132/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1520 - val_loss: 3.9955\n",
            "Epoch 133/500\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 4.2780 - val_loss: 4.0286\n",
            "Epoch 134/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3226 - val_loss: 3.9081\n",
            "Epoch 135/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0887 - val_loss: 3.9546\n",
            "Epoch 136/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.6041 - val_loss: 3.8811\n",
            "Epoch 137/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1027 - val_loss: 4.0149\n",
            "Epoch 138/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5023 - val_loss: 3.8093\n",
            "Epoch 139/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8635 - val_loss: 3.8068\n",
            "Epoch 140/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0762 - val_loss: 3.8649\n",
            "Epoch 141/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9642 - val_loss: 3.8873\n",
            "Epoch 142/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3531 - val_loss: 3.9421\n",
            "Epoch 143/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9394 - val_loss: 3.9781\n",
            "Epoch 144/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8540 - val_loss: 3.9326\n",
            "Epoch 145/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8505 - val_loss: 3.9791\n",
            "Epoch 146/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9599 - val_loss: 3.7991\n",
            "Epoch 147/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6850 - val_loss: 3.7768\n",
            "Epoch 148/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1267 - val_loss: 3.7195\n",
            "Epoch 149/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7504 - val_loss: 3.8490\n",
            "Epoch 150/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1247 - val_loss: 3.7765\n",
            "Epoch 151/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7597 - val_loss: 3.8805\n",
            "Epoch 152/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8818 - val_loss: 3.8547\n",
            "Epoch 153/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9249 - val_loss: 3.7996\n",
            "Epoch 154/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1114 - val_loss: 3.8630\n",
            "Epoch 155/500\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 4.3248 - val_loss: 4.1338\n",
            "Epoch 156/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2868 - val_loss: 3.8412\n",
            "Epoch 157/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9419 - val_loss: 4.0723\n",
            "Epoch 158/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2540 - val_loss: 4.2091\n",
            "Epoch 159/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9742 - val_loss: 3.9504\n",
            "Epoch 160/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7811 - val_loss: 3.9024\n",
            "Epoch 161/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.5133 - val_loss: 4.0015\n",
            "Epoch 162/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9659 - val_loss: 4.2001\n",
            "Epoch 163/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9152 - val_loss: 4.0067\n",
            "Epoch 164/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.4810 - val_loss: 4.0963\n",
            "Epoch 165/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1726 - val_loss: 4.2635\n",
            "Epoch 166/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0821 - val_loss: 4.1153\n",
            "Epoch 167/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5639 - val_loss: 4.2009\n",
            "Epoch 168/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3461 - val_loss: 4.1656\n",
            "Epoch 169/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0722 - val_loss: 4.2000\n",
            "Epoch 170/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8978 - val_loss: 4.1360\n",
            "Epoch 171/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3652 - val_loss: 4.2077\n",
            "Epoch 172/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1102 - val_loss: 4.0792\n",
            "Epoch 173/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9912 - val_loss: 4.1019\n",
            "Epoch 174/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6753 - val_loss: 4.0525\n",
            "Epoch 175/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.6182 - val_loss: 4.1431\n",
            "Epoch 176/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5736 - val_loss: 4.3404\n",
            "Epoch 177/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0648 - val_loss: 4.0844\n",
            "Epoch 178/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.6130 - val_loss: 4.1807\n",
            "Epoch 179/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.0108 - val_loss: 4.0642\n",
            "Epoch 180/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1335 - val_loss: 4.0530\n",
            "Epoch 181/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.4111 - val_loss: 4.2312\n",
            "Epoch 182/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.4233 - val_loss: 3.9227\n",
            "Epoch 183/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8414 - val_loss: 4.0502\n",
            "Epoch 184/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7595 - val_loss: 3.9115\n",
            "Epoch 185/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1871 - val_loss: 4.0158\n",
            "Epoch 186/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1179 - val_loss: 4.0849\n",
            "Epoch 187/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0571 - val_loss: 4.1054\n",
            "Epoch 188/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7047 - val_loss: 4.1198\n",
            "Epoch 189/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2472 - val_loss: 4.0171\n",
            "Epoch 190/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7316 - val_loss: 4.0120\n",
            "Epoch 191/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2625 - val_loss: 4.0327\n",
            "Epoch 192/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0391 - val_loss: 4.1705\n",
            "Epoch 193/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0888 - val_loss: 4.4528\n",
            "Epoch 194/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9569 - val_loss: 4.2060\n",
            "Epoch 195/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9657 - val_loss: 4.2364\n",
            "Epoch 196/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3331 - val_loss: 4.0272\n",
            "Epoch 197/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1727 - val_loss: 3.9915\n",
            "Epoch 198/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6995 - val_loss: 3.9867\n",
            "Epoch 199/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.9756 - val_loss: 3.9756\n",
            "Epoch 200/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2300 - val_loss: 4.4727\n",
            "Epoch 201/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1168 - val_loss: 4.3156\n",
            "Epoch 202/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0026 - val_loss: 4.5231\n",
            "Epoch 203/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1636 - val_loss: 4.0124\n",
            "Epoch 204/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3619 - val_loss: 4.1155\n",
            "Epoch 205/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6163 - val_loss: 4.1317\n",
            "Epoch 206/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2633 - val_loss: 4.1305\n",
            "Epoch 207/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2092 - val_loss: 4.1533\n",
            "Epoch 208/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2917 - val_loss: 4.1411\n",
            "Epoch 209/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9800 - val_loss: 4.2566\n",
            "Epoch 210/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9244 - val_loss: 4.0430\n",
            "Epoch 211/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1962 - val_loss: 4.3878\n",
            "Epoch 212/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.5718 - val_loss: 4.0696\n",
            "Epoch 213/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9739 - val_loss: 4.1212\n",
            "Epoch 214/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1567 - val_loss: 4.0471\n",
            "Epoch 215/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8658 - val_loss: 4.0599\n",
            "Epoch 216/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9996 - val_loss: 4.0583\n",
            "Epoch 217/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2497 - val_loss: 4.0978\n",
            "Epoch 218/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0813 - val_loss: 4.2198\n",
            "Epoch 219/500\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 4.1957 - val_loss: 4.0543\n",
            "Epoch 220/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2996 - val_loss: 3.9533\n",
            "Epoch 221/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9448 - val_loss: 3.9401\n",
            "Epoch 222/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8586 - val_loss: 3.8713\n",
            "Epoch 223/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6924 - val_loss: 4.1066\n",
            "Epoch 224/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7087 - val_loss: 3.8227\n",
            "Epoch 225/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0644 - val_loss: 3.9858\n",
            "Epoch 226/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9774 - val_loss: 4.0064\n",
            "Epoch 227/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1928 - val_loss: 3.9615\n",
            "Epoch 228/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6918 - val_loss: 3.9291\n",
            "Epoch 229/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0812 - val_loss: 3.9512\n",
            "Epoch 230/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0223 - val_loss: 3.9326\n",
            "Epoch 231/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.5485 - val_loss: 4.0974\n",
            "Epoch 232/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6999 - val_loss: 4.0923\n",
            "Epoch 233/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5482 - val_loss: 4.1280\n",
            "Epoch 234/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2265 - val_loss: 3.9490\n",
            "Epoch 235/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0497 - val_loss: 3.8239\n",
            "Epoch 236/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9026 - val_loss: 3.9498\n",
            "Epoch 237/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8956 - val_loss: 3.7847\n",
            "Epoch 238/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2294 - val_loss: 3.8200\n",
            "Epoch 239/500\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 3.9019 - val_loss: 3.8468\n",
            "Epoch 240/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8366 - val_loss: 4.2688\n",
            "Epoch 241/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9758 - val_loss: 3.8626\n",
            "Epoch 242/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9394 - val_loss: 3.9099\n",
            "Epoch 243/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3984 - val_loss: 3.9131\n",
            "Epoch 244/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.0731 - val_loss: 3.9251\n",
            "Epoch 245/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8419 - val_loss: 3.9570\n",
            "Epoch 246/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8068 - val_loss: 3.9643\n",
            "Epoch 247/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0391 - val_loss: 4.0853\n",
            "Epoch 248/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9499 - val_loss: 3.9962\n",
            "Epoch 249/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0354 - val_loss: 3.9956\n",
            "Epoch 250/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2340 - val_loss: 4.1090\n",
            "Epoch 251/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6157 - val_loss: 4.0944\n",
            "Epoch 252/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9027 - val_loss: 4.0636\n",
            "Epoch 253/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1382 - val_loss: 4.0791\n",
            "Epoch 254/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9849 - val_loss: 4.1012\n",
            "Epoch 255/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3149 - val_loss: 3.8881\n",
            "Epoch 256/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.6060 - val_loss: 3.8595\n",
            "Epoch 257/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1206 - val_loss: 4.2405\n",
            "Epoch 258/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3397 - val_loss: 4.0695\n",
            "Epoch 259/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8886 - val_loss: 3.9204\n",
            "Epoch 260/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.4739 - val_loss: 3.9364\n",
            "Epoch 261/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.0872 - val_loss: 3.9444\n",
            "Epoch 262/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.4472 - val_loss: 3.9691\n",
            "Epoch 263/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3747 - val_loss: 4.0682\n",
            "Epoch 264/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.5461 - val_loss: 3.9376\n",
            "Epoch 265/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8168 - val_loss: 4.0412\n",
            "Epoch 266/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0305 - val_loss: 4.1797\n",
            "Epoch 267/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3179 - val_loss: 4.0736\n",
            "Epoch 268/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2387 - val_loss: 4.0532\n",
            "Epoch 269/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2579 - val_loss: 3.9996\n",
            "Epoch 270/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.6137 - val_loss: 4.1742\n",
            "Epoch 271/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9280 - val_loss: 3.9818\n",
            "Epoch 272/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.4899 - val_loss: 4.0139\n",
            "Epoch 273/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2198 - val_loss: 3.9440\n",
            "Epoch 274/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0638 - val_loss: 4.0137\n",
            "Epoch 275/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0651 - val_loss: 4.0181\n",
            "Epoch 276/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2319 - val_loss: 3.9083\n",
            "Epoch 277/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7697 - val_loss: 3.8879\n",
            "Epoch 278/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8944 - val_loss: 3.8937\n",
            "Epoch 279/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9132 - val_loss: 3.8697\n",
            "Epoch 280/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0192 - val_loss: 3.8954\n",
            "Epoch 281/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.3982 - val_loss: 3.8296\n",
            "Epoch 282/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.8569 - val_loss: 3.6904\n",
            "Epoch 283/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9995 - val_loss: 3.6543\n",
            "Epoch 284/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8630 - val_loss: 3.7946\n",
            "Epoch 285/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2659 - val_loss: 3.7041\n",
            "Epoch 286/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8464 - val_loss: 3.9855\n",
            "Epoch 287/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9046 - val_loss: 3.6059\n",
            "Epoch 288/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.2693 - val_loss: 3.6727\n",
            "Epoch 289/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0756 - val_loss: 3.6933\n",
            "Epoch 290/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6750 - val_loss: 3.6788\n",
            "Epoch 291/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5740 - val_loss: 3.6545\n",
            "Epoch 292/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0698 - val_loss: 3.7166\n",
            "Epoch 293/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.4400 - val_loss: 3.5872\n",
            "Epoch 294/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8911 - val_loss: 3.7787\n",
            "Epoch 295/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1042 - val_loss: 3.7583\n",
            "Epoch 296/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3116 - val_loss: 3.7692\n",
            "Epoch 297/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0492 - val_loss: 3.9802\n",
            "Epoch 298/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2625 - val_loss: 3.9120\n",
            "Epoch 299/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1506 - val_loss: 3.9129\n",
            "Epoch 300/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.6787 - val_loss: 4.0228\n",
            "Epoch 301/500\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 3.7700 - val_loss: 3.9911\n",
            "Epoch 302/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6418 - val_loss: 3.8755\n",
            "Epoch 303/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9858 - val_loss: 3.9549\n",
            "Epoch 304/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0412 - val_loss: 4.1033\n",
            "Epoch 305/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0397 - val_loss: 4.1831\n",
            "Epoch 306/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7261 - val_loss: 4.0792\n",
            "Epoch 307/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3988 - val_loss: 4.0719\n",
            "Epoch 308/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.4686 - val_loss: 4.0325\n",
            "Epoch 309/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2576 - val_loss: 4.0082\n",
            "Epoch 310/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7429 - val_loss: 3.9104\n",
            "Epoch 311/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0582 - val_loss: 3.9405\n",
            "Epoch 312/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1336 - val_loss: 3.9251\n",
            "Epoch 313/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1186 - val_loss: 4.0456\n",
            "Epoch 314/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7580 - val_loss: 4.0626\n",
            "Epoch 315/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7305 - val_loss: 3.9755\n",
            "Epoch 316/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1906 - val_loss: 3.8958\n",
            "Epoch 317/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.3079 - val_loss: 3.8729\n",
            "Epoch 318/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.0583 - val_loss: 3.6197\n",
            "Epoch 319/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.1722 - val_loss: 4.1715\n",
            "Epoch 320/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7634 - val_loss: 3.7322\n",
            "Epoch 321/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.4126 - val_loss: 3.6140\n",
            "Epoch 322/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7465 - val_loss: 3.6417\n",
            "Epoch 323/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0993 - val_loss: 3.6556\n",
            "Epoch 324/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9558 - val_loss: 4.0186\n",
            "Epoch 325/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8065 - val_loss: 3.7313\n",
            "Epoch 326/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.5304 - val_loss: 3.7351\n",
            "Epoch 327/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3797 - val_loss: 3.7174\n",
            "Epoch 328/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.4573 - val_loss: 3.6816\n",
            "Epoch 329/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2659 - val_loss: 4.0362\n",
            "Epoch 330/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0562 - val_loss: 3.8797\n",
            "Epoch 331/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1406 - val_loss: 4.0192\n",
            "Epoch 332/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2460 - val_loss: 4.1595\n",
            "Epoch 333/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2378 - val_loss: 3.8925\n",
            "Epoch 334/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2952 - val_loss: 3.8516\n",
            "Epoch 335/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8593 - val_loss: 3.9679\n",
            "Epoch 336/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.0754 - val_loss: 3.8149\n",
            "Epoch 337/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1710 - val_loss: 3.8825\n",
            "Epoch 338/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3899 - val_loss: 3.9359\n",
            "Epoch 339/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3218 - val_loss: 3.9210\n",
            "Epoch 340/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3605 - val_loss: 4.4650\n",
            "Epoch 341/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9508 - val_loss: 4.1217\n",
            "Epoch 342/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.6891 - val_loss: 3.9576\n",
            "Epoch 343/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.5317 - val_loss: 3.9428\n",
            "Epoch 344/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2496 - val_loss: 3.9212\n",
            "Epoch 345/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0635 - val_loss: 4.1043\n",
            "Epoch 346/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3934 - val_loss: 4.0507\n",
            "Epoch 347/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.9714 - val_loss: 4.0609\n",
            "Epoch 348/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1892 - val_loss: 4.1776\n",
            "Epoch 349/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1532 - val_loss: 4.1004\n",
            "Epoch 350/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.5641 - val_loss: 4.0862\n",
            "Epoch 351/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1112 - val_loss: 4.0770\n",
            "Epoch 352/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2800 - val_loss: 4.2712\n",
            "Epoch 353/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9392 - val_loss: 4.1005\n",
            "Epoch 354/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0546 - val_loss: 4.1580\n",
            "Epoch 355/500\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 4.2649 - val_loss: 4.0577\n",
            "Epoch 356/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9980 - val_loss: 4.1062\n",
            "Epoch 357/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3764 - val_loss: 4.1619\n",
            "Epoch 358/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1355 - val_loss: 4.1494\n",
            "Epoch 359/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0475 - val_loss: 4.2564\n",
            "Epoch 360/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1979 - val_loss: 4.1225\n",
            "Epoch 361/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3479 - val_loss: 4.2050\n",
            "Epoch 362/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0677 - val_loss: 4.1342\n",
            "Epoch 363/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9756 - val_loss: 4.2715\n",
            "Epoch 364/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3553 - val_loss: 5.1989\n",
            "Epoch 365/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3420 - val_loss: 4.3794\n",
            "Epoch 366/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7823 - val_loss: 4.3658\n",
            "Epoch 367/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.1472 - val_loss: 4.2249\n",
            "Epoch 368/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9782 - val_loss: 4.2781\n",
            "Epoch 369/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8708 - val_loss: 4.0429\n",
            "Epoch 370/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0350 - val_loss: 4.0517\n",
            "Epoch 371/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.5417 - val_loss: 4.0638\n",
            "Epoch 372/500\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 4.4108 - val_loss: 4.2476\n",
            "Epoch 373/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.9433 - val_loss: 4.2015\n",
            "Epoch 374/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2834 - val_loss: 4.2105\n",
            "Epoch 375/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.6701 - val_loss: 4.1530\n",
            "Epoch 376/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1762 - val_loss: 4.4200\n",
            "Epoch 377/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2092 - val_loss: 4.1165\n",
            "Epoch 378/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.6328 - val_loss: 4.1494\n",
            "Epoch 379/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6553 - val_loss: 4.1193\n",
            "Epoch 380/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0441 - val_loss: 4.1717\n",
            "Epoch 381/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2413 - val_loss: 4.0378\n",
            "Epoch 382/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3056 - val_loss: 4.1391\n",
            "Epoch 383/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8128 - val_loss: 4.2401\n",
            "Epoch 384/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7690 - val_loss: 4.3356\n",
            "Epoch 385/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3506 - val_loss: 4.3141\n",
            "Epoch 386/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1030 - val_loss: 4.1961\n",
            "Epoch 387/500\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 4.2978 - val_loss: 4.2392\n",
            "Epoch 388/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8463 - val_loss: 4.0386\n",
            "Epoch 389/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2516 - val_loss: 3.9865\n",
            "Epoch 390/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9327 - val_loss: 3.9615\n",
            "Epoch 391/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.9128 - val_loss: 3.9875\n",
            "Epoch 392/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3086 - val_loss: 4.0009\n",
            "Epoch 393/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2764 - val_loss: 4.0115\n",
            "Epoch 394/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7423 - val_loss: 4.1827\n",
            "Epoch 395/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.9892 - val_loss: 4.2262\n",
            "Epoch 396/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.5330 - val_loss: 4.1260\n",
            "Epoch 397/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.9759 - val_loss: 4.1398\n",
            "Epoch 398/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0381 - val_loss: 4.0937\n",
            "Epoch 399/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.4120 - val_loss: 4.0477\n",
            "Epoch 400/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8530 - val_loss: 4.0567\n",
            "Epoch 401/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.6080 - val_loss: 4.0388\n",
            "Epoch 402/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.5465 - val_loss: 4.1446\n",
            "Epoch 403/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.5213 - val_loss: 4.0361\n",
            "Epoch 404/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7683 - val_loss: 4.0589\n",
            "Epoch 405/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3537 - val_loss: 4.0574\n",
            "Epoch 406/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1374 - val_loss: 3.9241\n",
            "Epoch 407/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9268 - val_loss: 3.9656\n",
            "Epoch 408/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0174 - val_loss: 3.9804\n",
            "Epoch 409/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2396 - val_loss: 3.9829\n",
            "Epoch 410/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.8227 - val_loss: 4.0146\n",
            "Epoch 411/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2248 - val_loss: 4.1220\n",
            "Epoch 412/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0321 - val_loss: 4.1164\n",
            "Epoch 413/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.8805 - val_loss: 4.2821\n",
            "Epoch 414/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.4563 - val_loss: 4.0618\n",
            "Epoch 415/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2046 - val_loss: 4.1042\n",
            "Epoch 416/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7238 - val_loss: 4.0966\n",
            "Epoch 417/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2408 - val_loss: 4.0954\n",
            "Epoch 418/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0179 - val_loss: 4.1607\n",
            "Epoch 419/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.2608 - val_loss: 4.1241\n",
            "Epoch 420/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8805 - val_loss: 4.2055\n",
            "Epoch 421/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7770 - val_loss: 4.2465\n",
            "Epoch 422/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.9701 - val_loss: 4.2350\n",
            "Epoch 423/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0429 - val_loss: 4.2141\n",
            "Epoch 424/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.1639 - val_loss: 4.1732\n",
            "Epoch 425/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0706 - val_loss: 4.2540\n",
            "Epoch 426/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1797 - val_loss: 3.9805\n",
            "Epoch 427/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9843 - val_loss: 4.1845\n",
            "Epoch 428/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.6439 - val_loss: 4.0030\n",
            "Epoch 429/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.3092 - val_loss: 4.0069\n",
            "Epoch 430/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7439 - val_loss: 4.0126\n",
            "Epoch 431/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8111 - val_loss: 4.0481\n",
            "Epoch 432/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.4240 - val_loss: 4.0141\n",
            "Epoch 433/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.1816 - val_loss: 3.9704\n",
            "Epoch 434/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.0387 - val_loss: 4.0234\n",
            "Epoch 435/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.3185 - val_loss: 4.2617\n",
            "Epoch 436/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.6111 - val_loss: 3.8543\n",
            "Epoch 437/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.5784 - val_loss: 3.8188\n",
            "Epoch 438/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1237 - val_loss: 3.9935\n",
            "Epoch 439/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1140 - val_loss: 3.9394\n",
            "Epoch 440/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1919 - val_loss: 4.0104\n",
            "Epoch 441/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7913 - val_loss: 3.8745\n",
            "Epoch 442/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1412 - val_loss: 3.8789\n",
            "Epoch 443/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9075 - val_loss: 3.9062\n",
            "Epoch 444/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1401 - val_loss: 3.8244\n",
            "Epoch 445/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3726 - val_loss: 3.7866\n",
            "Epoch 446/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.4267 - val_loss: 3.8239\n",
            "Epoch 447/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7209 - val_loss: 3.8852\n",
            "Epoch 448/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 3.8825 - val_loss: 4.0391\n",
            "Epoch 449/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3243 - val_loss: 4.0877\n",
            "Epoch 450/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.6276 - val_loss: 3.8293\n",
            "Epoch 451/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0806 - val_loss: 3.8104\n",
            "Epoch 452/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0369 - val_loss: 3.8187\n",
            "Epoch 453/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0212 - val_loss: 3.9078\n",
            "Epoch 454/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9171 - val_loss: 4.0892\n",
            "Epoch 455/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2375 - val_loss: 3.8921\n",
            "Epoch 456/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1761 - val_loss: 3.9348\n",
            "Epoch 457/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0063 - val_loss: 4.1758\n",
            "Epoch 458/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.4838 - val_loss: 3.7922\n",
            "Epoch 459/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0740 - val_loss: 4.0665\n",
            "Epoch 460/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.4796 - val_loss: 3.7813\n",
            "Epoch 461/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2906 - val_loss: 4.0343\n",
            "Epoch 462/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0633 - val_loss: 3.8710\n",
            "Epoch 463/500\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 3.7161 - val_loss: 3.9218\n",
            "Epoch 464/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.5972 - val_loss: 3.8305\n",
            "Epoch 465/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.8402 - val_loss: 4.1336\n",
            "Epoch 466/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.6443 - val_loss: 4.0930\n",
            "Epoch 467/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3066 - val_loss: 4.2100\n",
            "Epoch 468/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.4546 - val_loss: 4.2100\n",
            "Epoch 469/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1220 - val_loss: 4.2079\n",
            "Epoch 470/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0873 - val_loss: 4.1400\n",
            "Epoch 471/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0659 - val_loss: 4.0961\n",
            "Epoch 472/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0714 - val_loss: 4.1221\n",
            "Epoch 473/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1814 - val_loss: 3.9460\n",
            "Epoch 474/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0740 - val_loss: 4.4234\n",
            "Epoch 475/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3343 - val_loss: 4.7475\n",
            "Epoch 476/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.7320 - val_loss: 4.4277\n",
            "Epoch 477/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0179 - val_loss: 4.1838\n",
            "Epoch 478/500\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 4.2257 - val_loss: 4.2421\n",
            "Epoch 479/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7708 - val_loss: 4.3093\n",
            "Epoch 480/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.4172 - val_loss: 4.4339\n",
            "Epoch 481/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0634 - val_loss: 4.3050\n",
            "Epoch 482/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1619 - val_loss: 4.2594\n",
            "Epoch 483/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2472 - val_loss: 4.2031\n",
            "Epoch 484/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7083 - val_loss: 4.1286\n",
            "Epoch 485/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3616 - val_loss: 4.1316\n",
            "Epoch 486/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.7975 - val_loss: 4.1980\n",
            "Epoch 487/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.7921 - val_loss: 4.2873\n",
            "Epoch 488/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2413 - val_loss: 4.1740\n",
            "Epoch 489/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0466 - val_loss: 4.2401\n",
            "Epoch 490/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1657 - val_loss: 4.2682\n",
            "Epoch 491/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1354 - val_loss: 4.1345\n",
            "Epoch 492/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.2545 - val_loss: 4.0938\n",
            "Epoch 493/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9939 - val_loss: 4.1162\n",
            "Epoch 494/500\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 4.0765 - val_loss: 4.1655\n",
            "Epoch 495/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9732 - val_loss: 4.2962\n",
            "Epoch 496/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9914 - val_loss: 4.3160\n",
            "Epoch 497/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.3777 - val_loss: 4.3750\n",
            "Epoch 498/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.1205 - val_loss: 4.2209\n",
            "Epoch 499/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 4.0456 - val_loss: 4.1793\n",
            "Epoch 500/500\n",
            "15/15 [==============================] - 0s 1ms/step - loss: 3.9601 - val_loss: 4.1812\n",
            "5/5 [==============================] - 0s 1ms/step\n",
            "2/2 [==============================] - 0s 873us/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x29e905390>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas\n",
        "import tensorflow as tf\n",
        "\n",
        "### BEG DATA INTAKE\n",
        "# read data file and confirm data shape\n",
        "dataframe = pandas.read_csv('https://raw.githubusercontent.com/bryankolaczkowski/ALS3200C/main/phenopred.data.csv')\n",
        "print(dataframe.shape)\n",
        "\n",
        "# split into training and validation subsets, and confirm shape\n",
        "train_dataframe = dataframe.sample(frac=0.8, random_state=402201)\n",
        "valid_dataframe = dataframe.drop(train_dataframe.index)\n",
        "print(train_dataframe.shape, valid_dataframe.shape, dataframe.shape)\n",
        "\n",
        "# extract explanatory variables, convert to numpy and confirm shapes\n",
        "snp_ids = [ x for x in dataframe.columns if x.find('SNP') == 0 ]\n",
        "train_x = train_dataframe[snp_ids].to_numpy()\n",
        "valid_x = valid_dataframe[snp_ids].to_numpy()\n",
        "print(train_x.shape, valid_x.shape)\n",
        "\n",
        "# extract response variables, convert to numpy and confirm shapes\n",
        "train_y = train_dataframe['LS'].to_numpy()\n",
        "valid_y = valid_dataframe['LS'].to_numpy()\n",
        "print(train_y.shape, valid_y.shape)\n",
        "\n",
        "# package into tensorflow.Dataset objects and batch\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_x,train_y)).batch(10)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x,valid_y)).batch(36)\n",
        "### END DATA INTAKE\n",
        "\n",
        "### BEG NEURAL NETWORK TRAIN-VALIDATE\n",
        "## build and compile linear neural-network model\n",
        "model = tf.keras.models.Sequential()\n",
        "### BEG BUILD AND COMPILE MODEL\n",
        "model.add(tf.keras.layers.InputLayer(input_shape=[17165]))\n",
        "model.add(tf.keras.layers.Dropout(rate=0.9))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=tf.keras.losses.MeanAbsoluteError())\n",
        "### END BUILD AND COMPILE MODEL\n",
        "model.summary()\n",
        "\n",
        "## fit and validate neural-network model\n",
        "model.fit(train_data, epochs=500, validation_data=valid_data)\n",
        "### END NEURAL NETWORK TRAIN-VALIDATE\n",
        "\n",
        "### PLOT RESULTS FOR VISUALIZATION\n",
        "## predict training and validation responses for plotting\n",
        "train_y_hat = model.predict(train_x)\n",
        "valid_y_hat = model.predict(valid_x)\n",
        "\n",
        "## plot true-vs-predicted responses\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot([10,60],[10,60])\n",
        "plt.scatter(train_y, train_y_hat, marker='o')\n",
        "plt.scatter(valid_y, valid_y_hat, marker='+')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_RBlK9s0tUV"
      },
      "source": [
        "## Conclusions and Future Directions\n",
        "The preliminary neural network modeling conducted in this study provides early evidence to support a primarily additive genetic architecture underlying natural variation in Drosophila longevity phenotypes. The ability to achieve a reasonable fit using a simple linear model suggests longevity may be influenced by the cumulative small effects of many genes without substantial statistical epistasis.\n",
        "\n",
        "However, while aggressive input regularization via SNP dropout reduced overfitting in the context of limited sample sizes, the model has not yet been rigorously evaluated on fully independent test data. The constraints on data availability precluded partitioning into training, validation, and test sets.\n",
        "\n",
        "Follow-up studies should prioritize acquisition of additional sequenced fly lines to enable holding out a test set that can facilitate unbiased assessment of expected modeling performance on new data. This would provide greater certainty regarding the generalized predictive capacity beyond the training data.\n",
        "\n",
        "The extreme input dropout employed herein aided overfitting at the cost of reduced biological interpretability. As data resources expand, more modest regularization rates may help balance generalization with mechanistic elucidation.\n",
        "\n",
        "Future modeling should also evaluate more complex nonlinear neural network architectures to thoroughly survey the genotype-phenotype landscape. Comparing performance across a diverse array of models using cross-validation procedures could further validate the predominance of additive genetic effects.\n",
        "\n",
        "Expanding the diversity of sequenced Drosophila lines beyond the currently available inbred panels will be critical for enhancing biological relevance. Incorporating natural heterozygosity and population-level linkage patterns will help translate findings from the model system towards applications in outbred populations.\n",
        "\n",
        "In summary, this research demonstrates both the potential and current limitations of applying modern machine learning techniques to elucidate the genomic basis of complex traits. Continued efforts to expand open genomic data resources within the nourishing ecosystem of open science will help fully realize the promise of artificial intelligence approaches in biology."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "phenopred.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
